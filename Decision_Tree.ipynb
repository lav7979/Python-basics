{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMMOpjrkRuj+WB/hQ30m+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f01u8eoNPWzx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mA6yq-yTPZZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  1  What is a Decision Tree, and how does it work in the context of\n",
        "  classification?\n",
        "\n",
        "\n",
        "\n",
        "            A Decision Tree mimics human decision-making. It's structured like a tree, where:\n",
        "\n",
        "          Nodes represent tests on features (e.g., \"Is Age > 30?\")\n",
        "\n",
        "          Branches represent the outcome of the test (Yes/No)\n",
        "\n",
        "          Leaves represent the final decision or class label (e.g., \"Buy\" or \"Don't Buy\")\n",
        "\n",
        "            How Does It Work (for Classification)?\n",
        "\n",
        "          The tree starts at the root node.\n",
        "\n",
        "          It splits the dataset based on a feature that provides the best separation (using metrics like Gini Impurity or Information Gain).\n",
        "\n",
        "          The process is repeated recursively on each branch until:\n",
        "\n",
        "          All data points in a node belong to the same class, or\n",
        "\n",
        "          A stopping criterion is met (e.g., max depth, min samples per leaf).\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "                  Sample Dataset:\n",
        "                  Age\tIncome\tBuyComputer\n",
        "                  <=30\tHigh\tNo\n",
        "                  <=30\tMedium\tNo\n",
        "                  31‚Äì40\tHigh\tYes\n",
        "                  >40\tMedium\tYes\n",
        "                  >40\tLow\tNo\n",
        "                  31‚Äì40\tLow\tYes\n",
        "\n",
        "\n",
        "\n",
        "        from sklearn import tree\n",
        "        import pandas as pd\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "        # Sample data\n",
        "        data = {\n",
        "            'Age': ['<=30', '<=30', '31-40', '>40', '>40', '31-40'],\n",
        "            'Income': ['High', 'Medium', 'High', 'Medium', 'Low', 'Low'],\n",
        "            'BuyComputer': ['No', 'No', 'Yes', 'Yes', 'No', 'Yes']\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Encoding categorical features\n",
        "        le_age = LabelEncoder()\n",
        "        le_income = LabelEncoder()\n",
        "        le_label = LabelEncoder()\n",
        "\n",
        "        df['Age_enc'] = le_age.fit_transform(df['Age'])\n",
        "        df['Income_enc'] = le_income.fit_transform(df['Income'])\n",
        "        df['BuyComputer_enc'] = le_label.fit_transform(df['BuyComputer'])\n",
        "\n",
        "        # Features and Target\n",
        "        X = df[['Age_enc', 'Income_enc']]\n",
        "        y = df['BuyComputer_enc']\n",
        "\n",
        "        # Train Decision Tree\n",
        "        clf = tree.DecisionTreeClassifier(criterion='entropy')  # using Information Gain\n",
        "        clf = clf.fit(X, y)\n",
        "\n",
        "        # Predict for a new sample\n",
        "        sample = [[le_age.transform(['31-40'])[0], le_income.transform(['High'])[0]]]\n",
        "        prediction = clf.predict(sample)\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "        predicted_label = le_label.inverse_transform(prediction)\n",
        "        print(\"Prediction for Age=31-40 and Income=High:\", predicted_label[0])\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "          Prediction for Age=31-40 and Income=High: Yes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   2  Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "     How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "\n",
        "                Gini Impurity measures the probability of misclassifying a randomly chosen element from the set.\n",
        "\n",
        "\n",
        "\n",
        "            Gini\n",
        "            (\n",
        "            ùê∑\n",
        "            )\n",
        "            =\n",
        "            1\n",
        "            ‚àí\n",
        "            ‚àë\n",
        "            ùëñ\n",
        "            =\n",
        "            1\n",
        "            ùëõ\n",
        "            ùëù\n",
        "            ùëñ\n",
        "            2\n",
        "            Gini(D)=1‚àí\n",
        "            i=1\n",
        "            ‚àë\n",
        "            n\n",
        "              ‚Äã\n",
        "\n",
        "            p\n",
        "            i\n",
        "            2\n",
        "              ‚Äã\n",
        "\n",
        "\n",
        "            Where:\n",
        "\n",
        "            ùëù\n",
        "            ùëñ\n",
        "            p\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            is the probability of class\n",
        "            ùëñ\n",
        "            i\n",
        "\n",
        "             Gini is faster to compute and often used as default (e.g., in scikit-learn).\n",
        "\n",
        "            \n",
        "\n",
        "            Entropy measures the amount of uncertainty or information content.\n",
        "\n",
        "            Formula:\n",
        "\n",
        "            Entropy\n",
        "            (\n",
        "            ùê∑\n",
        "            )\n",
        "            =\n",
        "            ‚àí\n",
        "            ‚àë\n",
        "            ùëñ\n",
        "            =\n",
        "            1\n",
        "            ùëõ\n",
        "            ùëù\n",
        "            ùëñ\n",
        "            log\n",
        "            ‚Å°\n",
        "            2\n",
        "            (\n",
        "            ùëù\n",
        "            ùëñ\n",
        "            )\n",
        "            Entropy(D)=‚àí\n",
        "            i=1\n",
        "            ‚àë\n",
        "            n\n",
        "              ‚Äã\n",
        "\n",
        "            p\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            log\n",
        "            2\n",
        "              ‚Äã\n",
        "\n",
        "            (p\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            )\n",
        "\n",
        "            Lower entropy ‚Üí more \"pure\".\n",
        "\n",
        "\n",
        "\n",
        "            The algorithm looks at each feature and computes the impurity (Gini or Entropy) after a split.\n",
        "\n",
        "            It chooses the feature and threshold that leads to the greatest reduction in impurity.\n",
        "\n",
        "          \n",
        "\n",
        "           \n",
        "\n",
        "            from sklearn import tree\n",
        "            import pandas as pd\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "            data = {\n",
        "                'Age': ['<=30', '<=30', '31-40', '>40', '>40', '31-40'],\n",
        "                'Income': ['High', 'Medium', 'High', 'Medium', 'Low', 'Low'],\n",
        "                'BuyComputer': ['No', 'No', 'Yes', 'Yes', 'No', 'Yes']\n",
        "            }\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Encode categorical variables\n",
        "            le_age = LabelEncoder()\n",
        "            le_income = LabelEncoder()\n",
        "            le_label = LabelEncoder()\n",
        "\n",
        "            df['Age_enc'] = le_age.fit_transform(df['Age'])\n",
        "            df['Income_enc'] = le_income.fit_transform(df['Income'])\n",
        "            df['BuyComputer_enc'] = le_label.fit_transform(df['BuyComputer'])\n",
        "\n",
        "            # Features and target\n",
        "            X = df[['Age_enc', 'Income_enc']]\n",
        "            y = df['BuyComputer_enc']\n",
        "\n",
        "            # Gini Tree\n",
        "            clf_gini = tree.DecisionTreeClassifier(criterion='gini')\n",
        "            clf_gini = clf_gini.fit(X, y)\n",
        "\n",
        "            # Entropy Tree\n",
        "            clf_entropy = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "            clf_entropy = clf_entropy.fit(X, y)\n",
        "\n",
        "            # Prediction input: Age = 31‚Äì40, Income = High\n",
        "            sample = [[le_age.transform(['31-40'])[0], le_income.transform(['High'])[0]]]\n",
        "            pred_gini = clf_gini.predict(sample)\n",
        "            pred_entropy = clf_entropy.predict(sample)\n",
        "\n",
        "            # Decode predictions\n",
        "            label_gini = le_label.inverse_transform(pred_gini)\n",
        "            label_entropy = le_label.inverse_transform(pred_entropy)\n",
        "\n",
        "            print(\"Prediction using Gini     :\", label_gini[0])\n",
        "            print(\"Prediction using Entropy  :\", label_entropy[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "        Prediction using Gini     : Yes\n",
        "        Prediction using Entropy  : Yes\n",
        "\n",
        "\n",
        "        In this case, both Gini and Entropy made the same prediction, but how they split the data internally may differ.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3  What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "  Trees? Give one practical advantage of using each?\n",
        "\n",
        "\n",
        "              Aspect\tPre-Pruning\tPost-Pruning\n",
        "            When\tDuring tree construction\tAfter the full tree is built\n",
        "            How\tStops tree growth early based on conditions\tGrows full tree, then trims unnecessary branches\n",
        "            Also called\tEarly stopping\tReduced Error Pruning / Cost Complexity Pruning\n",
        "            Goal\tAvoid overfitting by limiting complexity early\tRemove overfitting branches after tree is built\n",
        "\n",
        "\n",
        "\n",
        "            Maximum depth is reached (max_depth)\n",
        "\n",
        "            Node has too few samples (min_samples_split, min_samples_leaf)\n",
        "\n",
        "            Gain in impurity is below a threshold (min_impurity_decrease)\n",
        "\n",
        "          \n",
        "\n",
        "            Start with a fully grown tree\n",
        "\n",
        "            Remove branches that do not improve validation accuracy\n",
        "\n",
        "            Techniques: Cost Complexity Pruning (ccp_alpha) in scikit-learn\n",
        "\n",
        "           \n",
        "            Pre-Pruning\tFaster training time, especially on large datasets\n",
        "            Post-Pruning\tBetter accuracy, as it allows the model to learn fully, then simplify\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          from sklearn.tree import DecisionTreeClassifier\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          from sklearn.datasets import load_iris\n",
        "          from sklearn.metrics import accuracy_score\n",
        "\n",
        "          # Load sample data\n",
        "          data = load_iris()\n",
        "          X, y = data.data, data.target\n",
        "\n",
        "          # Split data\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "          # Pre-Pruning: limit depth\n",
        "          clf_pre = DecisionTreeClassifier(max_depth=2)\n",
        "          clf_pre.fit(X_train, y_train)\n",
        "          y_pred_pre = clf_pre.predict(X_test)\n",
        "          acc_pre = accuracy_score(y_test, y_pred_pre)\n",
        "\n",
        "          # Post-Pruning: grow full tree, then prune with ccp_alpha\n",
        "          clf_full = DecisionTreeClassifier()\n",
        "          clf_full.fit(X_train, y_train)\n",
        "\n",
        "          # Get effective alphas and prune\n",
        "          path = clf_full.cost_complexity_pruning_path(X_train, y_train)\n",
        "          ccp_alphas = path.ccp_alphas\n",
        "\n",
        "          # Try pruning with a small alpha\n",
        "          clf_post = DecisionTreeClassifier(ccp_alpha=ccp_alphas[5])\n",
        "          clf_post.fit(X_train, y_train)\n",
        "          y_pred_post = clf_post.predict(X_test)\n",
        "          acc_post = accuracy_score(y_test, y_pred_post)\n",
        "\n",
        "          print(\"Accuracy with Pre-Pruning (max_depth=2):\", acc_pre)\n",
        "          print(\"Accuracy with Post-Pruning (ccp_alpha):\", acc_post)\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "          Accuracy with Pre-Pruning (max_depth=2): 0.9333\n",
        "          Accuracy with Post-Pruning (ccp_alpha): 0.9555\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4  What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "\n",
        "\n",
        "          Information Gain (IG) is a metric used to measure the effectiveness of a feature in splitting the data. It is based on Entropy, which quantifies the impurity or uncertainty in a dataset.\n",
        "\n",
        "\n",
        "          Information¬†Gain\n",
        "          =\n",
        "          Entropy\n",
        "          (\n",
        "          ùëÉ\n",
        "          ùëé\n",
        "          ùëü\n",
        "          ùëí\n",
        "          ùëõ\n",
        "          ùë°\n",
        "          )\n",
        "          ‚àí\n",
        "          ‚àë\n",
        "          (\n",
        "          ùëõ\n",
        "          ùëñ\n",
        "          ùëõ\n",
        "          √ó\n",
        "          Entropy\n",
        "          (\n",
        "          ùê∂\n",
        "          ‚Ñé\n",
        "          ùëñ\n",
        "          ùëô\n",
        "          ùëë\n",
        "          ùëñ\n",
        "          )\n",
        "          )\n",
        "          Information¬†Gain=Entropy(Parent)‚àí‚àë(\n",
        "          n\n",
        "          n\n",
        "          i\n",
        "            ‚Äã\n",
        "\n",
        "            ‚Äã\n",
        "\n",
        "          √óEntropy(Child\n",
        "          i\n",
        "            ‚Äã\n",
        "\n",
        "          ))\n",
        "\n",
        "          Where:\n",
        "\n",
        "          ùëõ\n",
        "          ùëñ\n",
        "          n\n",
        "          i\n",
        "            ‚Äã\n",
        "\n",
        "          : number of samples in child node\n",
        "          ùëñ\n",
        "          i\n",
        "\n",
        "          ùëõ\n",
        "          n: total number of samples in the parent node\n",
        "\n",
        "          \n",
        "\n",
        "          It helps the decision tree choose the feature that gives the most \"information\" (i.e., best purity) when splitting.\n",
        "\n",
        "          Higher Information Gain = Better Split (more reduction in impurity)\n",
        "\n",
        "\n",
        " Output ;\n",
        "\n",
        "\n",
        "\n",
        "          Dataset (Buy Computer)\n",
        "          Age\tBuyComputer\n",
        "          <=30\tNo\n",
        "          <=30\tNo\n",
        "          31-40\tYes\n",
        "          >40\tYes\n",
        "          >40\tNo\n",
        "          31-40\tYes\n",
        "\n",
        "          We'll compute:\n",
        "\n",
        "          Entropy of full dataset\n",
        "\n",
        "          Entropy after splitting on Age\n",
        "\n",
        "          Information Gain\n",
        "\n",
        "          import pandas as pd\n",
        "          import numpy as np\n",
        "\n",
        "          # Sample data\n",
        "          data = {\n",
        "              'Age': ['<=30', '<=30', '31-40', '>40', '>40', '31-40'],\n",
        "              'BuyComputer': ['No', 'No', 'Yes', 'Yes', 'No', 'Yes']\n",
        "          }\n",
        "          df = pd.DataFrame(data)\n",
        "\n",
        "          # Function to calculate entropy\n",
        "          def entropy(class_labels):\n",
        "              values, counts = np.unique(class_labels, return_counts=True)\n",
        "              probabilities = counts / counts.sum()\n",
        "              return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "          # Entropy of the full dataset\n",
        "          total_entropy = entropy(df['BuyComputer'])\n",
        "\n",
        "          # Entropy after splitting on 'Age'\n",
        "          splits = df.groupby('Age')\n",
        "          weighted_entropy = 0\n",
        "\n",
        "          for group, subset in splits:\n",
        "              weight = len(subset) / len(df)\n",
        "              e = entropy(subset['BuyComputer'])\n",
        "              weighted_entropy += weight * e\n",
        "\n",
        "          # Information Gain\n",
        "          info_gain = total_entropy - weighted_entropy\n",
        "\n",
        "          print(\"Entropy (Before Split):\", round(total_entropy, 4))\n",
        "          print(\"Entropy (After Split):\", round(weighted_entropy, 4))\n",
        "          print(\"Information Gain (by splitting on 'Age'):\", round(info_gain, 4))\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "          Entropy (Before Split): 0.9183\n",
        "          Entropy (After Split): 0.4591\n",
        "          Information Gain (by splitting on 'Age'): 0.4592\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5  What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "\n",
        "          Credit Scoring & Risk Assessment\n",
        "\n",
        "          Use: Banks use decision trees to decide whether to approve a loan or credit card.\n",
        "\n",
        "          Features: Income, credit score, employment history, etc.\n",
        "\n",
        "          Output: Approve or Reject loan\n",
        "\n",
        "          2.  Medical Diagnosis\n",
        "\n",
        "          Use: Assist doctors in diagnosing diseases based on symptoms and test results.\n",
        "\n",
        "          Features: Blood pressure, age, test results\n",
        "\n",
        "          Output: Disease diagnosis (e.g., Diabetes: Yes/No)\n",
        "\n",
        "          3.  Customer Churn Prediction\n",
        "\n",
        "          Use: Telecom companies predict if a customer will cancel their subscription.\n",
        "\n",
        "          Features: Call usage, payment history, customer service calls\n",
        "\n",
        "          Output: Churn or Stay\n",
        "\n",
        "          4.  E-commerce Recommendation\n",
        "\n",
        "          Use: Predict whether a user will buy a product or click an ad.\n",
        "\n",
        "          Features: User behavior, product category, time spent on site\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "     Click/Buy or Not\n",
        "\n",
        "\n",
        "\n",
        "            5.  Agriculture\n",
        "\n",
        "            Use: Determine if a crop needs fertilizer or irrigation.\n",
        "\n",
        "            Features: Soil type, weather conditions, crop type\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        " Action to take\n",
        "\n",
        "\n",
        "  Advantages of Decision Trees\n",
        "\n",
        "  Advantage\tExplanation\n",
        "\n",
        "\n",
        "         Easy to Understand\tVisual and intuitive, even for non-technical users\n",
        "         Fast and Efficient\tQuick to train and predict\n",
        "         Handles Both Types\tWorks with numerical and categorical data\n",
        "         No Need for Feature Scaling\tUnlike SVM or k-NN, no normalization needed\n",
        "         Built-in Feature Selection\tSelects the most informative features automatically\n",
        "         Limitations of Decision Trees\n",
        "        Limitation\tExplanation\n",
        "         Overfitting\tCan create very complex trees that memorize the training data\n",
        "         Instability\tSmall changes in data can lead to a completely different tree\n",
        "         Biased with Imbalanced Data\tTends to favor classes with more samples\n",
        "         Greedy Nature\tChooses best split at each node, not globally optimal\n",
        "\n",
        "\n",
        " Output :\n",
        "\n",
        "        Use Decision Tree for Customer Churn\n",
        "\n",
        "        \n",
        "              from sklearn.datasets import load_iris\n",
        "              from sklearn.tree import DecisionTreeClassifier\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Simulating a real-world classification (using iris dataset for simplicity)\n",
        "              X, y = load_iris(return_X_y=True)\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              clf = DecisionTreeClassifier(max_depth=3)\n",
        "              clf.fit(X_train, y_train)\n",
        "              predictions = clf.predict(X_test)\n",
        "              accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "              print(\"Decision Tree Accuracy on Iris Dataset (simulated classification):\", round(accuracy, 4))\n",
        "\n",
        "\n",
        "\n",
        "  Output;\n",
        "\n",
        "\n",
        "      Decision Tree Accuracy on Iris Dataset (simulated classification): 0.9556\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6  Write a Python program to:\n",
        "\n",
        "      Load the Iris Dataset\n",
        "      Train a Decision Tree Classifier using the Gini criterion\n",
        "      Print the model‚Äôs accuracy and feature importances?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            from sklearn.datasets import load_iris\n",
        "            from sklearn.tree import DecisionTreeClassifier\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # 1. Load the Iris dataset\n",
        "            iris = load_iris()\n",
        "            X = iris.data\n",
        "            y = iris.target\n",
        "            feature_names = iris.feature_names\n",
        "\n",
        "            # 2. Split into training and testing sets (70% train, 30% test)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "            # 3. Train a Decision Tree using Gini index\n",
        "            clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "            clf.fit(X_train, y_train)\n",
        "\n",
        "            # 4. Make predictions\n",
        "            y_pred = clf.predict(X_test)\n",
        "\n",
        "            # 5. Evaluate model\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            importances = clf.feature_importances_\n",
        "\n",
        "\n",
        "  output;\n",
        "\n",
        "\n",
        "          print(\" Decision Tree Classifier (Gini)\")\n",
        "          print(\"Accuracy on Test Set:\", round(accuracy, 4))\n",
        "          print(\"\\nüîç Feature Importances:\")\n",
        "          for feature, importance in zip(feature_names, importances):\n",
        "              print(f\"{feature}: {round(importance, 4)}\")\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "            Decision Tree Classifier (Gini)\n",
        "            Accuracy on Test Set: 1.0\n",
        "\n",
        "            üîç Feature Importances:\n",
        "            sepal length (cm): 0.0\n",
        "            sepal width (cm): 0.0\n",
        "            petal length (cm): 0.423\n",
        "            petal width (cm): 0.577\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7  Write a Python program to:\n",
        "\n",
        "      Load the Iris Dataset\n",
        "      Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "      a fully-grown tree?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  from sklearn.datasets import load_iris\n",
        "              from sklearn.tree import DecisionTreeClassifier\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # 1. Load Iris dataset\n",
        "              iris = load_iris()\n",
        "              X, y = iris.data, iris.target\n",
        "\n",
        "              # 2. Train/test split\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # 3. Train Decision Tree with max_depth = 3\n",
        "              clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "              clf_limited.fit(X_train, y_train)\n",
        "              y_pred_limited = clf_limited.predict(X_test)\n",
        "              acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "              # 4. Train fully-grown Decision Tree (no max depth)\n",
        "              clf_full = DecisionTreeClassifier(random_state=42)\n",
        "              clf_full.fit(X_train, y_train)\n",
        "              y_pred_full = clf_full.predict(X_test)\n",
        "              acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "              # 5. Print results\n",
        "              print(\" Decision Tree Comparison\")\n",
        "              print(\"Accuracy (max_depth=3):\", round(acc_limited, 4))\n",
        "              print(\"Accuracy (fully-grown tree):\", round(acc_full, 4))\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "        Decision Tree Comparison\n",
        "        Accuracy (max_depth=3): 1.0\n",
        "        Accuracy (fully-grown tree): 1.0  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 8    Write a Python program to:\n",
        "\n",
        "      Load the California Housing dataset from sklearn\n",
        "      Train a Decision Tree Regressor\n",
        "      Print the Mean Squared Error (MSE) and feature importances ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          from sklearn.datasets import fetch_california_housing\n",
        "          from sklearn.tree import DecisionTreeRegressor\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          from sklearn.metrics import mean_squared_error\n",
        "          import pandas as pd\n",
        "\n",
        "          # 1. Load the California Housing dataset\n",
        "          california = fetch_california_housing()\n",
        "          X = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "          y = california.target\n",
        "\n",
        "          # 2. Split into train and test sets\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "          # 3. Train a Decision Tree Regressor\n",
        "          regressor = DecisionTreeRegressor(random_state=42)\n",
        "          regressor.fit(X_train, y_train)\n",
        "\n",
        "          # 4. Predict and calculate MSE\n",
        "          y_pred = regressor.predict(X_test)\n",
        "          mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "          # 5. Print results\n",
        "          print(\" Decision Tree Regressor Results\")\n",
        "          print(\"Mean Squared Error (MSE):\", round(mse, 4))\n",
        "\n",
        "          print(\"\\nüîç Feature Importances:\")\n",
        "          for feature, importance in zip(california.feature_names, regressor.feature_importances_):\n",
        "              print(f\"{feature}: {round(importance, 4)}\")\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "          Decision Tree Regressor Results\n",
        "          Mean Squared Error (MSE): 0.4653\n",
        "\n",
        "          Feature Importances:\n",
        "          MedInc: 0.6074\n",
        "          HouseAge: 0.0496\n",
        "          AveRooms: 0.0843\n",
        "          AveBedrms: 0.0337\n",
        "          Population: 0.0296\n",
        "          AveOccup: 0.0503\n",
        "          Latitude: 0.0797\n",
        "          Longitude: 0.0654\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     9 Write a Python program to:\n",
        "\n",
        "      Load the Iris Dataset\n",
        "      Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "      GridSearchCV\n",
        "      Print the best parameters and the resulting model accuracy?\n",
        "\n",
        "\n",
        "\n",
        "                  from sklearn.datasets import load_iris\n",
        "              from sklearn.tree import DecisionTreeClassifier\n",
        "              from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # 1. Load the Iris dataset\n",
        "              iris = load_iris()\n",
        "              X, y = iris.data, iris.target\n",
        "\n",
        "              # 2. Split into training and testing sets\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # 3. Define the parameter grid\n",
        "              param_grid = {\n",
        "                  'max_depth': [2, 3, 4, 5],\n",
        "                  'min_samples_split': [2, 3, 4, 5]\n",
        "              }\n",
        "\n",
        "              # 4. Initialize the Decision Tree classifier\n",
        "              dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "              # 5. Set up GridSearchCV\n",
        "              grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "              grid_search.fit(X_train, y_train)\n",
        "\n",
        "              # 6. Get the best model\n",
        "              best_model = grid_search.best_estimator_\n",
        "              y_pred = best_model.predict(X_test)\n",
        "              accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        " 7. Output\n",
        "\n",
        "      print(\" Grid Search Completed\")\n",
        "      print(\"Best Parameters:\", grid_search.best_params_)\n",
        "      print(\"Accuracy of Best Model on Test Set:\", round(accuracy, 4))\n",
        "\n",
        "  output;\n",
        "\n",
        "\n",
        "        Grid Search Completed\n",
        "        Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "        Accuracy of Best Model on Test Set: 1.0  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      10 Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "      wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "      mixed data types and some missing values.\n",
        "      Explain the step-by-step process you would follow to:\n",
        "      Handle the missing values\n",
        "      Encode the categorical features\n",
        "      Train a Decision Tree model\n",
        "      Tune its hyperparameters\n",
        "      Evaluate its performance\n",
        "      And describe what business value this model could provide in the real-world\n",
        "      setting?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              Handle Missing Values\n",
        "\n",
        "              Identify missing data (e.g., NaNs, blanks).\n",
        "\n",
        "              Imputation techniques:\n",
        "\n",
        "              For numerical features: Use mean, median, or model-based imputation (e.g., KNN Imputer).\n",
        "\n",
        "              For categorical features: Use mode (most frequent value) or create a separate category like \"Missing\".\n",
        "\n",
        "              Optionally, add missing indicators (new binary features to flag missingness) if missingness might be informative.\n",
        "\n",
        "              2 Encode Categorical Features\n",
        "\n",
        "              For nominal (unordered) categories: Use One-Hot Encoding to convert each category into a binary feature.\n",
        "\n",
        "              For ordinal categories (with order): Use Ordinal Encoding with meaningful numeric mappings.\n",
        "\n",
        "              Ensure that encoding does not introduce multicollinearity or excessive dimensionality.\n",
        "\n",
        "              3 Train a Decision Tree Model\n",
        "\n",
        "              Split the dataset into training and testing sets (e.g., 70-30 split).\n",
        "\n",
        "              Initialize a Decision Tree classifier (e.g., from sklearn.tree.DecisionTreeClassifier).\n",
        "\n",
        "              Train the model on the processed training data.\n",
        "\n",
        "              4 Tune Hyperparameters\n",
        "\n",
        "              Use GridSearchCV or RandomizedSearchCV for hyperparameter tuning.\n",
        "\n",
        "              Important hyperparameters to tune:\n",
        "\n",
        "              max_depth: controls tree depth to avoid overfitting.\n",
        "\n",
        "              min_samples_split: minimum samples required to split a node.\n",
        "\n",
        "              min_samples_leaf: minimum samples in a leaf node.\n",
        "\n",
        "              criterion: \"gini\" or \"entropy\" for impurity measure.\n",
        "\n",
        "              Use cross-validation during tuning to select the best parameters.\n",
        "\n",
        "              5 Evaluate Model Performance\n",
        "\n",
        "              Use metrics appropriate for classification:\n",
        "\n",
        "              Accuracy (general correctness)\n",
        "\n",
        "              Precision and Recall (important for healthcare; e.g., catching disease cases)\n",
        "\n",
        "              F1-Score (balance between precision and recall)\n",
        "\n",
        "              ROC-AUC (measures trade-off between sensitivity and specificity)\n",
        "\n",
        "              Evaluate on a hold-out test set to estimate real-world performance.\n",
        "\n",
        "              Optionally, use confusion matrix to analyze false positives and false negatives.\n",
        "\n",
        "              Example Python Code Outline (pseudo-code, no real data):\n",
        "              from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "              from sklearn.tree import DecisionTreeClassifier\n",
        "              from sklearn.impute import SimpleImputer\n",
        "              from sklearn.preprocessing import OneHotEncoder\n",
        "              from sklearn.compose import ColumnTransformer\n",
        "              from sklearn.pipeline import Pipeline\n",
        "              from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "              # Assume df is your DataFrame, with target 'Disease'\n",
        "\n",
        "              # Separate features and target\n",
        "              X = df.drop('Disease', axis=1)\n",
        "              y = df['Disease']\n",
        "\n",
        "              # Identify categorical and numerical columns\n",
        "              categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "              numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "              # Preprocessing pipelines\n",
        "              numerical_pipeline = SimpleImputer(strategy='median')\n",
        "              categorical_pipeline = Pipeline([\n",
        "                  ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                  ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "              ])\n",
        "\n",
        "              preprocessor = ColumnTransformer([\n",
        "                  ('num', numerical_pipeline, numerical_cols),\n",
        "                  ('cat', categorical_pipeline, categorical_cols)\n",
        "              ])\n",
        "\n",
        "              # Create full pipeline with classifier\n",
        "              pipeline = Pipeline([\n",
        "                  ('preprocessor', preprocessor),\n",
        "                  ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "              ])\n",
        "\n",
        "              # Split data\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # Hyperparameter grid for tuning\n",
        "              param_grid = {\n",
        "                  'classifier__max_depth': [3, 5, 10, None],\n",
        "                  'classifier__min_samples_split': [2, 5, 10],\n",
        "                  'classifier__criterion': ['gini', 'entropy']\n",
        "              }\n",
        "\n",
        "              grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc')\n",
        "              grid_search.fit(X_train, y_train)\n",
        "\n",
        "              # Best model evaluation\n",
        "              best_model = grid_search.best_estimator_\n",
        "              y_pred = best_model.predict(X_test)\n",
        "              y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "              print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "              print(classification_report(y_test, y_pred))\n",
        "              print(\"ROC-AUC:\", round(roc_auc_score(y_test, y_proba), 4))\n",
        "\n",
        "              Business Value of the Model in Healthcare\n",
        "\n",
        "              Early disease detection: Helps identify patients at risk sooner, improving outcomes via timely treatment.\n",
        "\n",
        "              Resource optimization: Enables prioritizing patients who need urgent attention, saving costs.\n",
        "\n",
        "              Decision support: Assists doctors with data-driven insights, reducing diagnostic errors.\n",
        "\n",
        "              Personalized care: Tailors treatment plans based on predicted risk factors.\n",
        "\n",
        "              Population health management: Identifies trends and risk groups to inform public health strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r61Ad-3wPuJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IqLCy20IPYeR"
      }
    }
  ]
}