{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfxaX86y0vyTCN+1CYwOiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tFK7sQjccNqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1  What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "\n",
        "      Linear Regression\n",
        "\n",
        "                Purpose:\n",
        "                Used for predicting a continuous dependent variable (output).\n",
        "\n",
        "                Example:\n",
        "                Predicting house prices based on square footage.\n",
        "\n",
        "                Mathematical Form:\n",
        "\n",
        "                ğ‘¦\n",
        "                =\n",
        "                ğ›½\n",
        "                0\n",
        "                +\n",
        "                ğ›½\n",
        "                1\n",
        "                ğ‘¥\n",
        "                +\n",
        "                ğœ–\n",
        "                y=Î²\n",
        "                0\n",
        "                  â€‹\n",
        "\n",
        "                +Î²\n",
        "                1\n",
        "                  â€‹\n",
        "\n",
        "                x+Ïµ\n",
        "\n",
        "                Where:\n",
        "\n",
        "                ğ‘¦\n",
        "                y is the predicted continuous value,\n",
        "\n",
        "                ğ‘¥\n",
        "                x is the input feature(s),\n",
        "\n",
        "                ğ›½\n",
        "                0\n",
        "                ,\n",
        "                ğ›½\n",
        "                1\n",
        "                Î²\n",
        "                0\n",
        "                  â€‹\n",
        "\n",
        "                ,Î²\n",
        "                1\n",
        "                  â€‹\n",
        "\n",
        "                are coefficients,\n",
        "\n",
        "                ğœ–\n",
        "                Ïµ is the error term.\n",
        "\n",
        "\n",
        "\n",
        "     Logistic Regression\n",
        "\n",
        "              Purpose:\n",
        "              Used for classification problems, typically binary (0 or 1), but can be extended to multi-class.\n",
        "\n",
        "              Example:\n",
        "              Predicting whether an email is spam (1) or not spam (0).\n",
        "\n",
        "              Mathematical Form:\n",
        "              Instead of predicting a raw value, it predicts the probability of the outcome being class 1:\n",
        "\n",
        "              ğ‘ƒ\n",
        "              (\n",
        "              ğ‘¦\n",
        "              =\n",
        "              1\n",
        "              âˆ£\n",
        "              ğ‘¥\n",
        "              )\n",
        "              =\n",
        "              1\n",
        "              1\n",
        "              +\n",
        "              ğ‘’\n",
        "              âˆ’\n",
        "              (\n",
        "              ğ›½\n",
        "              0\n",
        "              +\n",
        "              ğ›½\n",
        "              1\n",
        "              ğ‘¥\n",
        "              )\n",
        "              P(y=1âˆ£x)=\n",
        "              1+e\n",
        "              âˆ’(Î²\n",
        "              0\n",
        "                â€‹\n",
        "\n",
        "              +Î²\n",
        "              1\n",
        "                â€‹\n",
        "\n",
        "              x)\n",
        "              1\n",
        "                â€‹\n",
        "\n",
        "\n",
        "              This function is called the sigmoid or logistic function, which maps input values to a range between 0 and 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     Key Differences\n",
        "\n",
        "        Feature\tLinear Regression\tLogistic Regression\n",
        "        Output Type\tContinuous\tProbability (0 to 1)\n",
        "        Target Variable\tNumeric\tCategorical (usually binary)\n",
        "        Algorithm Output\tA number\tA probability, then mapped to a class\n",
        "        Function Used\tLinear function\tSigmoid/logistic function\n",
        "        Loss Function\tMean Squared Error (MSE)\tLog Loss (Cross-Entropy)\n",
        "        Use Case\tRegression problems\tClassification problems\n",
        "\n",
        "\n",
        "  Visual Example\n",
        "\n",
        "\n",
        "        Linear Regression: Produces a straight line best fit.\n",
        "\n",
        "        Logistic Regression: Produces an S-shaped curve (sigmoid).\n",
        "\n",
        "\n",
        "\n",
        "  2  Explain the role of the Sigmoid function in Logistic Regression?      \n",
        "\n",
        "\n",
        "\n",
        "                    Role of the Sigmoid Function in Logistic Regression\n",
        "\n",
        "                In logistic regression, the sigmoid function is used to convert the output of a linear equation into a probability value between 0 and 1, which is essential for binary classification tasks.\n",
        "\n",
        "                 1. Transforms Linear Output to Probability\n",
        "\n",
        "                Logistic regression starts by computing a linear combination of input features:\n",
        "\n",
        "                ğ‘§\n",
        "                =\n",
        "                ğ›½\n",
        "                0\n",
        "                +\n",
        "                ğ›½\n",
        "                1\n",
        "                ğ‘¥\n",
        "                1\n",
        "                +\n",
        "                ğ›½\n",
        "                2\n",
        "                ğ‘¥\n",
        "                2\n",
        "                +\n",
        "                â‹¯\n",
        "                +\n",
        "                ğ›½\n",
        "                ğ‘›\n",
        "                ğ‘¥\n",
        "                ğ‘›\n",
        "                z=Î²\n",
        "                0\n",
        "                  â€‹\n",
        "\n",
        "                +Î²\n",
        "                1\n",
        "                  â€‹\n",
        "\n",
        "                x\n",
        "                1\n",
        "                  â€‹\n",
        "\n",
        "                +Î²\n",
        "                2\n",
        "                  â€‹\n",
        "\n",
        "                x\n",
        "                2\n",
        "                  â€‹\n",
        "\n",
        "                +â‹¯+Î²\n",
        "                n\n",
        "                  â€‹\n",
        "\n",
        "                x\n",
        "                n\n",
        "                  â€‹\n",
        "\n",
        "\n",
        "                This\n",
        "                ğ‘§\n",
        "                z can be any real number (from\n",
        "                âˆ’\n",
        "                âˆ\n",
        "                âˆ’âˆ to\n",
        "                +\n",
        "                âˆ\n",
        "                +âˆ).\n",
        "\n",
        "                To interpret this as a probability, we pass\n",
        "                ğ‘§\n",
        "                z through the sigmoid function:\n",
        "\n",
        "                ğœ\n",
        "                (\n",
        "                ğ‘§\n",
        "                )\n",
        "                =\n",
        "                1\n",
        "                1\n",
        "                +\n",
        "                ğ‘’\n",
        "                âˆ’\n",
        "                ğ‘§\n",
        "                Ïƒ(z)=\n",
        "                1+e\n",
        "                âˆ’z\n",
        "                1\n",
        "                  â€‹\n",
        "\n",
        "\n",
        "                This squashes the output to a range between 0 and 1, turning it into a valid probability.\n",
        "\n",
        "                 2. Enables Binary Classification\n",
        "\n",
        "                If\n",
        "                ğœ\n",
        "                (\n",
        "                ğ‘§\n",
        "                )\n",
        "                â‰¥\n",
        "                0.5\n",
        "                Ïƒ(z)â‰¥0.5, classify the input as class 1 (positive).\n",
        "\n",
        "                If\n",
        "                ğœ\n",
        "                (\n",
        "                ğ‘§\n",
        "                )\n",
        "                <\n",
        "                0.5\n",
        "                Ïƒ(z)<0.5, classify the input as class 0 (negative).\n",
        "\n",
        "                So, the sigmoid function allows logistic regression to make decisions based on computed probabilities.\n",
        "\n",
        "                ğŸ”¸ 3. Supports Gradient-Based Optimization\n",
        "\n",
        "                The sigmoid function is:\n",
        "\n",
        "                Differentiable, allowing gradient descent to be used for training.\n",
        "\n",
        "                Has a clean derivative:\n",
        "\n",
        "                ğœ\n",
        "                â€²\n",
        "                (\n",
        "                ğ‘§\n",
        "                )\n",
        "                =\n",
        "                ğœ\n",
        "                (\n",
        "                ğ‘§\n",
        "                )\n",
        "                (\n",
        "                1\n",
        "                âˆ’\n",
        "                ğœ\n",
        "                (\n",
        "                ğ‘§\n",
        "                )\n",
        "                )\n",
        "                Ïƒ\n",
        "                â€²\n",
        "                (z)=Ïƒ(z)(1âˆ’Ïƒ(z))\n",
        "\n",
        "                This simplifies computations during optimization.\n",
        "                \n",
        "\n",
        "                 In Summary:\n",
        "                Role of Sigmoid Function\tWhy It's Important\n",
        "                Converts linear output to probability\tNeeded for classification\n",
        "                Outputs values between 0 and 1\tMakes predictions interpretable\n",
        "                Differentiable\tSupports training via gradient descent\n",
        "                Helps define decision boundary\tThreshold at 0.5 for binary classification  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   3   What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "\n",
        "\n",
        "                  Logistic Regression tries to find the best-fitting decision boundary by learning weights (coefficients).\n",
        "\n",
        "\n",
        "          If the model has too many features or if the data is noisy, it may overfit â€” i.e., perform well on training data but poorly on unseen (test) data.\n",
        "\n",
        "\n",
        "          Regularization penalizes large coefficients, which helps the model generalize better to new data.\n",
        "\n",
        "\n",
        "           How Regularization Works\n",
        "\n",
        "\n",
        "          The standard loss function in logistic regression is the log loss (or binary cross-entropy). Regularization adds a penalty term to this loss:\n",
        "\n",
        "          ğ½\n",
        "          (\n",
        "          ğ‘¤\n",
        "          )\n",
        "          =\n",
        "          LogÂ Loss\n",
        "          +\n",
        "          ğœ†\n",
        "          â‹…\n",
        "          RegularizationÂ Term\n",
        "          J(w)=LogÂ Loss+Î»â‹…RegularizationÂ Term\n",
        "\n",
        "          Where:\n",
        "\n",
        "          ğ½\n",
        "          (\n",
        "          ğ‘¤\n",
        "          )\n",
        "          J(w) is the new cost function,\n",
        "\n",
        "          ğœ†\n",
        "          Î» is the regularization strength (hyperparameter),\n",
        "\n",
        "          The regularization term depends on the type used (L1 or L2).\n",
        "\n",
        "            Types of Regularization\n",
        "\n",
        "          Type\tName\tPenalty Term\tEffect\n",
        "          L1\tLasso\t( \\lambda \\sum\tw_j\n",
        "          L2\tRidge\n",
        "          ğœ†\n",
        "          âˆ‘\n",
        "          ğ‘¤\n",
        "          ğ‘—\n",
        "          2\n",
        "          Î»âˆ‘w\n",
        "          j\n",
        "          2\n",
        "            â€‹\n",
        "\n",
        "            Shrinks weights smoothly, keeps all features but reduces impact\n",
        "\n",
        "          Logistic regression with L2 regularization is very common â€” many libraries (like sklearn) apply it by default.\n",
        "\n",
        "\n",
        "           Summary\n",
        "\n",
        "\n",
        "          Regularization helps logistic regression avoid overfitting.\n",
        "\n",
        "          It controls model complexity by adding a penalty for large weights.\n",
        "\n",
        "          You can use L1 (for feature selection) or L2 (for general shrinkage).\n",
        "\n",
        "          The regularization strength is controlled by a hyperparameter (e.g., C in scikit-learn, where smaller C = stronger regularization).\n",
        "\n",
        "\n",
        "\n",
        "    4 What are some common evaluation metrics for classification models, and\n",
        "    why are they importan?\n",
        "\n",
        "\n",
        "            Common Evaluation Metrics for Classification:\n",
        "\n",
        "          1. Accuracy\n",
        "\n",
        "          Definition: The proportion of total correct predictions.\n",
        "\n",
        "          Formula:\n",
        "\n",
        "          Accuracy\n",
        "          =\n",
        "          ğ‘‡\n",
        "          ğ‘ƒ\n",
        "          +\n",
        "          ğ‘‡\n",
        "          ğ‘\n",
        "          ğ‘‡\n",
        "          ğ‘ƒ\n",
        "          +\n",
        "          ğ‘‡\n",
        "          ğ‘\n",
        "          +\n",
        "          ğ¹\n",
        "          ğ‘ƒ\n",
        "          +\n",
        "          ğ¹\n",
        "          ğ‘\n",
        "          Accuracy=\n",
        "          TP+TN+FP+FN\n",
        "          TP+TN\n",
        "            â€‹\n",
        "\n",
        "\n",
        "          Use When: Classes are balanced.\n",
        "\n",
        "          Limitation: Misleading for imbalanced datasets (e.g., predicting 95% accuracy on a dataset where 95% of examples are one class).\n",
        "\n",
        "          2. Precision\n",
        "\n",
        "          Definition: Out of all predicted positives, how many were actually positive?\n",
        "\n",
        "          Formula:\n",
        "\n",
        "          Precision\n",
        "          =\n",
        "          ğ‘‡\n",
        "          ğ‘ƒ\n",
        "          ğ‘‡\n",
        "          ğ‘ƒ\n",
        "          +\n",
        "          ğ¹\n",
        "          ğ‘ƒ\n",
        "          Precision=\n",
        "          TP+FP\n",
        "          TP\n",
        "            â€‹\n",
        "\n",
        "\n",
        "          Use When: False positives are costly (e.g., spam detection).\n",
        "\n",
        "          3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "          Definition: Out of all actual positives, how many were correctly predicted?\n",
        "\n",
        "          Formula:\n",
        "\n",
        "          Recall\n",
        "          =\n",
        "          ğ‘‡\n",
        "          ğ‘ƒ\n",
        "          ğ‘‡\n",
        "          ğ‘ƒ\n",
        "          +\n",
        "          ğ¹\n",
        "          ğ‘\n",
        "          Recall=\n",
        "          TP+FN\n",
        "          TP\n",
        "            â€‹\n",
        "\n",
        "\n",
        "          Use When: False negatives are costly (e.g., medical diagnosis).\n",
        "\n",
        "          4. F1 Score\n",
        "\n",
        "          Definition: Harmonic mean of Precision and Recall.\n",
        "\n",
        "          Formula:\n",
        "\n",
        "          F1\n",
        "          =\n",
        "          2\n",
        "          â‹…\n",
        "          Precision\n",
        "          â‹…\n",
        "          Recall\n",
        "          Precision\n",
        "          +\n",
        "          Recall\n",
        "          F1=2â‹…\n",
        "          Precision+Recall\n",
        "          Precisionâ‹…Recall\n",
        "            â€‹\n",
        "\n",
        "\n",
        "          Use When: You need a balance between precision and recall.\n",
        "\n",
        "          5. ROC Curve and AUC (Area Under the Curve)\n",
        "\n",
        "          ROC Curve: Plots True Positive Rate vs. False Positive Rate at different thresholds.\n",
        "\n",
        "          AUC: Measures the entire area under the ROC curve (0.5 = random, 1.0 = perfect).\n",
        "\n",
        "          Use When: You want to evaluate model performance across all thresholds.\n",
        "\n",
        "          6. Confusion Matrix\n",
        "\n",
        "          Definition: A table showing TP, TN, FP, FN counts.\n",
        "\n",
        "          Helps understand the types of errors your model is making.\n",
        "\n",
        "\n",
        "           Why These Metrics Matter:\n",
        "\n",
        "\n",
        "          Not all problems are the same. For example:\n",
        "\n",
        "          In fraud detection: high precision may be critical.\n",
        "\n",
        "          In disease screening: high recall is often more important.\n",
        "\n",
        "          Accuracy alone can be deceptive, especially with imbalanced data.\n",
        "\n",
        "          Choosing the right metric helps you align the modelâ€™s performance with business or domain goals.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  5  Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "  splits into train/test sets, trains a Logistic Regression model, and prints its accuracy?\n",
        "\n",
        "\n",
        "        import pandas as pd\n",
        "        from sklearn.datasets import load_breast_cancer\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        from sklearn.metrics import accuracy_score\n",
        "\n",
        "        # 1. Load the dataset\n",
        "\n",
        "        data = load_breast_cancer()\n",
        "        df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        df['target'] = data.target\n",
        "\n",
        "        # 2. Split into features and target\n",
        "\n",
        "        X = df.drop('target', axis=1)\n",
        "        y = df['target']\n",
        "\n",
        "        # 3. Split into train and test sets (80% train, 20% test)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # 4. Train a Logistic Regression model\n",
        "\n",
        "        model = LogisticRegression(max_iter=10000)  # Increase max_iter if convergence warning appears\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # 5. Predict and evaluate accuracy\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
        "\n",
        "         Notes:\n",
        "\n",
        "        max_iter=10000 ensures the model converges. You can reduce this if convergence is quick.\n",
        "\n",
        "        This uses a binary classification problem, ideal for Logistic Regression.\n",
        "\n",
        "        You can replace load_breast_cancer() with other datasets like load_iris() or load_digits() for variety.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  6   Write a Python program to train a Logistic Regression model using L2\n",
        "     regularization (Ridge) and print the model coefficients and accuracy?\n",
        "\n",
        "\n",
        "\n",
        "                  import pandas as pd\n",
        "              from sklearn.datasets import load_iris\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.linear_model import LogisticRegression\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # 1. Load the dataset\n",
        "              data = load_iris()\n",
        "              df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "              df['target'] = data.target\n",
        "\n",
        "              # 2. Split features and target\n",
        "              X = df.drop('target', axis=1)\n",
        "              y = df['target']\n",
        "\n",
        "              # 3. Train-test split\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "              # 4. Train Logistic Regression with L2 regularization\n",
        "              model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', multi_class='auto', max_iter=1000)\n",
        "              model.fit(X_train, y_train)\n",
        "\n",
        "              # 5. Predictions and accuracy\n",
        "              y_pred = model.predict(X_test)\n",
        "              accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "# . Output\n",
        "\n",
        "              print(\"Model Coefficients (one row per class):\")\n",
        "              print(model.coef_)\n",
        "\n",
        "              print(\"\\nIntercepts:\")\n",
        "              print(model.intercept_)\n",
        "\n",
        "              print(f\"\\nAccuracy on test set: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "â„¹ Explanation:\n",
        "\n",
        "\n",
        "          penalty='l2': Specifies L2 regularization (Ridge).\n",
        "\n",
        "          C=1.0: Inverse of regularization strength (lower = stronger regularization).\n",
        "\n",
        "          model.coef_: Returns weights for each feature per class.\n",
        "\n",
        "          model.intercept_: Bias term(s) for each class.\n",
        "\n",
        "          accuracy_score: Measures the fraction of correct predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    7 Write a Python program to train a Logistic Regression model for multiclass\n",
        "    classification using multi_class='ovr' and print the classification report?\n",
        "\n",
        "\n",
        "                import pandas as pd\n",
        "            from sklearn.datasets import load_iris\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.linear_model import LogisticRegression\n",
        "            from sklearn.metrics import classification_report\n",
        "\n",
        "            # 1. Load the dataset\n",
        "            data = load_iris()\n",
        "            df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "            df['target'] = data.target\n",
        "\n",
        "            # 2. Features and target\n",
        "            X = df.drop('target', axis=1)\n",
        "            y = df['target']\n",
        "\n",
        "            # 3. Train-test split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # 4. Train Logistic Regression with One-vs-Rest strategy\n",
        "            model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # 5. Predict and print classification report\n",
        "            y_pred = model.predict(X_test)\n",
        "            print(\"Classification Report:\\n\")\n",
        "            print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "          The classification_report will include:\n",
        "\n",
        "          Precision: Out of all predicted for a class, how many were correct.\n",
        "\n",
        "          Recall: Out of all actual for a class, how many were captured.\n",
        "\n",
        "          F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "          Support: Number of actual occurrences of the class in the test set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      8  Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "      hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "      accuracy?\n",
        "\n",
        "\n",
        "\n",
        "              import pandas as pd\n",
        "              from sklearn.datasets import load_iris\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.linear_model import LogisticRegression\n",
        "              from sklearn.metrics import classification_report\n",
        "\n",
        "              # 1. Load the dataset\n",
        "              data = load_iris()\n",
        "              df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "              df['target'] = data.target\n",
        "\n",
        "              # 2. Features and target\n",
        "              X = df.drop('target', axis=1)\n",
        "              y = df['target']\n",
        "\n",
        "              # 3. Train-test split\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "              # 4. Train Logistic Regression with One-vs-Rest strategy\n",
        "              model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "              model.fit(X_train, y_train)\n",
        "\n",
        "              # 5. Predict and print classification report\n",
        "              y_pred = model.predict(X_test)\n",
        "              print(\"Classification Report:\\n\")\n",
        "              print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n",
        "         Output:\n",
        "\n",
        "\n",
        "        The classification_report will include:\n",
        "\n",
        "        Precision: Out of all predicted for a class, how many were correct.\n",
        "\n",
        "        Recall: Out of all actual for a class, how many were captured.\n",
        "\n",
        "        F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "        Support: Number of actual occurrences of the class in the test set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    9 Write a Python program to standardize the features before training Logistic\n",
        "    Regression and compare the model's accuracy with and without scaling?\n",
        "\n",
        "\n",
        "\n",
        "                import pandas as pd\n",
        "            from sklearn.datasets import load_iris\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.linear_model import LogisticRegression\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # 1. Load dataset\n",
        "            data = load_iris()\n",
        "            df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "            df['target'] = data.target\n",
        "\n",
        "            # 2. Split features and target\n",
        "            X = df.drop('target', axis=1)\n",
        "            y = df['target']\n",
        "\n",
        "            # 3. Train/test split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # ----------------------\n",
        "            # Model without scaling\n",
        "            # ----------------------\n",
        "            model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "            model_no_scaling.fit(X_train, y_train)\n",
        "            y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "            acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "            # ----------------------\n",
        "            # Model with scaling\n",
        "            # ----------------------\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            model_scaled = LogisticRegression(max_iter=1000)\n",
        "            model_scaled.fit(X_train_scaled, y_train)\n",
        "            y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "            acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "\n",
        " Compare results\n",
        "\n",
        "\n",
        "      print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n",
        "      print(f\"Accuracy with scaling:    {acc_scaled:.4f}\")\n",
        "\n",
        "\n",
        " Explanation:\n",
        "\n",
        "\n",
        "      StandardScaler transforms features to have mean = 0 and std = 1.\n",
        "\n",
        "      Logistic Regression may converge faster and generalize better with scaled inputs, especially when features are on very different scales.\n",
        "\n",
        "      We compare test set accuracy with and without scaling.\n",
        "\n",
        "\n",
        "\n",
        "        10  Imagine you are working at an e-commerce company that wants to\n",
        "      predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "      dataset (only 5% of customers respond), describe the approach youâ€™d take to build a\n",
        "      Logistic Regression model â€” including data handling, feature scaling, balancing\n",
        "      classes, hyperparameter tuning, and evaluating the model for this real-world business?\n",
        "\n",
        "\n",
        "\n",
        "      Understand the Business Context\n",
        "\n",
        "\n",
        "              Goal: Maximize the number of positive responders correctly identified (maximize precision/recall).\n",
        "\n",
        "              Constraint: Misclassifying non-responders as responders may waste marketing resources; missing actual responders is a missed opportunity.\n",
        "\n",
        "              Metric Importance: Focus on metrics like Precision, Recall, F1-score, and AUC-ROC, rather than just Accuracy.\n",
        "\n",
        "              2. Data Exploration & Cleaning\n",
        "\n",
        "              Check for missing values and handle them (impute with mean/median/mode or drop).\n",
        "\n",
        "              Analyze class distribution (i.e., ~5% responders, 95% non-responders).\n",
        "\n",
        "              Identify and remove data leakage (features that wonâ€™t be available at prediction time).\n",
        "\n",
        "              3. Feature Engineering & Scaling\n",
        "\n",
        "              Encoding categorical variables: Use one-hot encoding or ordinal encoding depending on the type.\n",
        "\n",
        "              Feature scaling: Since Logistic Regression assumes features are on similar scales, apply StandardScaler (mean=0, std=1) or MinMaxScaler.\n",
        "\n",
        "              4. Handling Class Imbalance\n",
        "\n",
        "              Given only 5% responders, the dataset is severely imbalanced. This needs correction for a well-performing logistic model.\n",
        "\n",
        "              Options:\n",
        "\n",
        "              Resampling\n",
        "\n",
        "              Oversample minority class (e.g., SMOTE, ADASYN).\n",
        "\n",
        "              Undersample majority class (e.g., RandomUnderSampler).\n",
        "\n",
        "              Or combine both (balanced sampling).\n",
        "\n",
        "              Class weights\n",
        "\n",
        "              Set class_weight='balanced' in scikit-learnâ€™s LogisticRegression, which adjusts the loss function to penalize misclassification of the minority class more heavily.\n",
        "\n",
        "              âš ï¸ Tip: Start with class_weight='balanced', as it is simple and works well with Logistic Regression.\n",
        "\n",
        "              5. Model Training\n",
        "\n",
        "              Use Stratified K-Fold Cross-Validation to ensure each fold has a similar class distribution.\n",
        "\n",
        "              from sklearn.linear_model import LogisticRegression\n",
        "              from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "              from sklearn.preprocessing import StandardScaler\n",
        "              from sklearn.pipeline import Pipeline\n",
        "\n",
        "              pipeline = Pipeline([\n",
        "                  ('scaler', StandardScaler()),\n",
        "                  ('lr', LogisticRegression(class_weight='balanced', solver='liblinear'))\n",
        "              ])\n",
        "\n",
        "              6. Hyperparameter Tuning\n",
        "\n",
        "\n",
        "              Use GridSearchCV or RandomizedSearchCV with stratified folds:\n",
        "\n",
        "              Key hyperparameters:\n",
        "\n",
        "              C: Regularization strength (inverse of lambda); smaller values imply stronger regularization.\n",
        "\n",
        "              penalty: 'l1' or 'l2'\n",
        "\n",
        "              param_grid = {\n",
        "                  'lr__C': [0.001, 0.01, 0.1, 1, 10],\n",
        "                  'lr__penalty': ['l1', 'l2']\n",
        "              }\n",
        "\n",
        "              cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "              grid = GridSearchCV(pipeline, param_grid, scoring='f1', cv=cv)\n",
        "              grid.fit(X_train, y_train)\n",
        "\n",
        "              7. Model Evaluation\n",
        "\n",
        "              Since the data is imbalanced, accuracy is misleading. Evaluate using:\n",
        "\n",
        "              Confusion Matrix\n",
        "\n",
        "              Precision, Recall, F1-score\n",
        "\n",
        "              ROC-AUC\n",
        "\n",
        "              PR-AUC (Precision-Recall AUC) â€” especially informative when positives are rare\n",
        "\n",
        "              Lift/Gain charts (optional but good for marketing)\n",
        "\n",
        "              from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
        "\n",
        "              y_pred = grid.predict(X_test)\n",
        "              y_proba = grid.predict_proba(X_test)[:,1]\n",
        "\n",
        "              print(classification_report(y_test, y_pred))\n",
        "              print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "              precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "              pr_auc = auc(recall, precision)\n",
        "              print(\"PR-AUC:\", pr_auc)\n",
        "\n",
        "              8. Threshold Tuning\n",
        "\n",
        "              Default threshold is 0.5, but you may want to lower it to catch more responders (higher recall).\n",
        "\n",
        "              Use Precision-Recall trade-off to select an operating point based on business goals (e.g., minimum acceptable precision).\n",
        "\n",
        "              9. Business Interpretation\n",
        "\n",
        "              Probabilities output by Logistic Regression can help rank customers by likelihood to respond.\n",
        "\n",
        "              Use this to create a target list for campaigns â€” top N% likely to respond.\n",
        "\n",
        "              Use lift metrics to show how much better the model performs than random targeting.\n",
        "\n",
        "              10. Deployment Considerations\n",
        "\n",
        "              Monitor for data drift â€” update model periodically.\n",
        "\n",
        "              Track conversion rate of predicted responders to validate ROI.\n",
        "\n",
        "              Integrate into existing CRM or marketing platform for targeting.\n",
        "\n",
        "\n",
        "              Summary of Key Tactics\n",
        "\n",
        "              Step\tTechnique\n",
        "              Class Imbalance\tclass_weight='balanced', SMOTE\n",
        "              Scaling\tStandardScaler()\n",
        "              Evaluation\tF1, ROC-AUC, PR-AUC\n",
        "              Hyperparameters\tC, penalty via GridSearchCV\n",
        "              Threshold tuning\tAdjust decision threshold based on business goals.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                          import numpy as np\n",
        "            import pandas as pd\n",
        "            from sklearn.datasets import make_classification\n",
        "\n",
        "            # Simulate imbalanced dataset: 5% responders\n",
        "            X, y = make_classification(n_samples=10000, n_features=20,\n",
        "                                      n_informative=5, n_redundant=2,\n",
        "                                      weights=[0.95, 0.05], flip_y=0,\n",
        "                                      random_state=42)\n",
        "\n",
        "            # Convert to DataFrame for clarity\n",
        "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "            y = pd.Series(y, name='responded')\n",
        "\n",
        "            # Check class balance\n",
        "            print(y.value_counts(normalize=True))\n",
        "\n",
        " Output:\n",
        "\n",
        "                  0    0.95\n",
        "                  1    0.05\n",
        "                  Name: responded, dtype: float64\n",
        "\n",
        "\n",
        "2.  Train/Test Split\n",
        "\n",
        "\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "        3. âš–ï¸ Handle Class Imbalance (class_weight='balanced')\n",
        "        We'll use class weighting to compensate for the imbalance.\n",
        "\n",
        "\n",
        "4.  Feature Scaling + Modeling Pipeline\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(class_weight='balanced', solver='liblinear'))\n",
        "])\n",
        "\n",
        "5.  Hyperparameter Tuning with GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'lr__C': [0.01, 0.1, 1, 10],\n",
        "    'lr__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipeline, param_grid, scoring='f1', cv=5)\n",
        "grid.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "yZ087__GcPNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LIhBi3_Yh5qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F8xSw4v4hA-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeRfvL1cbRqX"
      },
      "outputs": [],
      "source": []
    }
  ]
}