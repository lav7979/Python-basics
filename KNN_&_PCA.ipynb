{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvuxy05Hdbm5mbRVAb8okQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/KNN_%26_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cugAqvc4Pvij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m7FRh6tKQb_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "\n",
        "            Definition of KNN:\n",
        "\n",
        "            K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression. It is instance-based and non-parametric, meaning it does not make any underlying assumptions about the data distribution.\n",
        "\n",
        "             How KNN Works:\n",
        "\n",
        "            Choose the number of neighbors (K).\n",
        "\n",
        "            Calculate the distance between the new data point and all other points in the dataset (commonly using Euclidean distance).\n",
        "\n",
        "            Select the K closest points (neighbors).\n",
        "\n",
        "            Make a prediction:\n",
        "\n",
        "            Classification: Majority vote of the K neighbors’ labels.\n",
        "\n",
        "            Regression: Average of the K neighbors’ values.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "               Dataset (Simplified):\n",
        "              Age\tSalary\tBuys Computer\n",
        "              25\t50000\tYes\n",
        "              30\t60000\tYes\n",
        "              35\t70000\tNo\n",
        "              40\t80000\tNo\n",
        "               New Data: Age=28, Salary=52000\n",
        "              Step-by-Step (K=3):\n",
        "\n",
        "              Calculate Distance to all existing points:\n",
        "\n",
        "              Point A (25,50000) → √((28-25)² + (52000-50000)²) ≈ small\n",
        "\n",
        "              ...\n",
        "\n",
        "              Select 3 Nearest Neighbors (e.g., all \"Yes\", \"Yes\", \"No\")\n",
        "\n",
        "              Majority Voting:\n",
        "\n",
        "              2 Yes, 1 No → Predict: \"Yes\"\n",
        "\n",
        "\n",
        "\n",
        " Output :\n",
        "\n",
        "              Predicted Class: Yes\n",
        "\n",
        "               Example 2: KNN for Regression\n",
        "               Dataset:\n",
        "              Size (sqft)\tPrice ($)\n",
        "              1000\t200000\n",
        "              1200\t240000\n",
        "              1500\t300000\n",
        "              1800\t360000\n",
        "               New House Size: 1300 sqft\n",
        "              Step-by-Step (K=2):\n",
        "\n",
        "              Calculate Distances:\n",
        "\n",
        "              To 1200 → 100\n",
        "\n",
        "              To 1500 → 200\n",
        "\n",
        "              (Nearest are 1200 and 1500)\n",
        "\n",
        "              Average the Prices of Nearest Neighbors:\n",
        "\n",
        "              (240000 + 300000) / 2 = 270000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                Output;\n",
        "\n",
        "\n",
        "\n",
        "                Predicted Price: $270,000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2 What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "\n",
        "\n",
        "            The Curse of Dimensionality refers to various issues that arise when working with high-dimensional data (i.e., data with a large number of features). As dimensions increase:\n",
        "\n",
        "            The data becomes sparse.\n",
        "\n",
        "            Distance metrics lose meaning.\n",
        "\n",
        "            Algorithms like KNN become less effective.\n",
        "\n",
        "             How It Affects KNN:\n",
        "\n",
        "            KNN relies heavily on distance calculations (like Euclidean distance) to find the \"nearest\" neighbors. But in high dimensions:\n",
        "\n",
        "            All points start to look equally far away.\n",
        "\n",
        "            Noise and irrelevant features dilute meaningful patterns.\n",
        "\n",
        "            Increased computation time due to more dimensions.\n",
        "\n",
        "             Example: Effect of Dimensions on KNN Distance\n",
        "\n",
        "\n",
        "\n",
        "              Let’s simulate what happens when we increase dimensions while keeping data points random.\n",
        "\n",
        "               Scenario:\n",
        "\n",
        "              Two points in 1D vs 100D\n",
        "\n",
        "              Use Euclidean distance\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "  Python Simulation\n",
        "\n",
        "\n",
        "\n",
        "                  import numpy as np\n",
        "\n",
        "                  # Generate two random points\n",
        "                  np.random.seed(42)\n",
        "\n",
        "                  dims = [1, 5, 10, 50, 100, 500]\n",
        "                  for d in dims:\n",
        "                      point1 = np.random.rand(d)\n",
        "                      point2 = np.random.rand(d)\n",
        "                      distance = np.linalg.norm(point1 - point2)\n",
        "                      print(f\"Distance in {d}D: {distance:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "              Distance in 1D: 0.2509\n",
        "              Distance in 5D: 1.1067\n",
        "              Distance in 10D: 1.4548\n",
        "              Distance in 50D: 2.8864\n",
        "              Distance in 100D: 4.1792\n",
        "              Distance in 500D: 9.0547\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "\n",
        "        Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a large set of possibly correlated features into a smaller set of uncorrelated variables called principal components.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                Input Dataset:\n",
        "                Feature 1\tFeature 2\tFeature 3\n",
        "                2.5\t2.4\t3.5\n",
        "                0.5\t0.7\t1.1\n",
        "                2.2\t2.9\t3.3\n",
        "                1.9\t2.2\t2.9\n",
        "                 Goal: Reduce 3 features to 2 using PCA\n",
        "\n",
        "\n",
        "\n",
        " Output :\n",
        "\n",
        "\n",
        "            PC1\tPC2\n",
        "            0.82797\t-0.17512\n",
        "            -1.77758\t-0.14243\n",
        "            0.99219\t0.38437\n",
        "            0.25242\t0.02525\n",
        "\n",
        "          Now, instead of 3 features, we have 2 principal components that still retain most of the variance in the original data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " PCA vs Feature Selection\n",
        "\n",
        "          Aspect\tPCA (Dimensionality Reduction)\tFeature Selection\n",
        "          What it does\tCreates new features (principal components)\tSelects a subset of existing features\n",
        "          Based on\tVariance and correlation\tRelevance to the target variable\n",
        "          Features used\tTransformed (linear combinations of original features)\tOriginal features only\n",
        "          Interpretability\tLess interpretable (components are abstract)\tHigh (original feature names retained)\n",
        "          Supervised?\t Unsupervised (ignores output label)\t Often supervised (can use label info)\n",
        "          Purpose\tReduce dimensionality while retaining variance\tImprove performance, reduce overfitting, simplify\n",
        "\n",
        " Code Example: PCA in Python\n",
        "\n",
        "\n",
        "                    from sklearn.decomposition import PCA\n",
        "                    from sklearn.datasets import load_iris\n",
        "                    import pandas as pd\n",
        "\n",
        "                    # Load Iris dataset\n",
        "                    data = load_iris()\n",
        "                    X = data.data\n",
        "                    features = data.feature_names\n",
        "\n",
        "                    # Apply PCA to reduce from 4D to 2D\n",
        "                    pca = PCA(n_components=2)\n",
        "                    X_pca = pca.fit_transform(X)\n",
        "\n",
        "                    # Show transformed data\n",
        "                    df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
        "                    print(df_pca.head())\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "\n",
        "                                  PC1       PC2\n",
        "                            0  -2.6841   0.3194\n",
        "                            1  -2.7141  -0.1770\n",
        "                            2  -2.8889  -0.1449\n",
        "                            3  -2.7453  -0.3183\n",
        "                            4  -2.7287   0.3268\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "\n",
        "\n",
        "        In Principal Component Analysis (PCA), eigenvalues and eigenvectors play a crucial role in transforming the data into a new coordinate system where the axes (called principal components) maximize the variance in the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    Eigenvectors:\n",
        "              These are directions in the feature space along which the data varies the most. In PCA, these directions are the principal components.\n",
        "\n",
        "              Eigenvalues:\n",
        "              These indicate the amount of variance (i.e., information) captured by each principal component (eigenvector). A higher eigenvalue means more variance is captured in that direction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "                            Imagine a 2D dataset:\n",
        "\n",
        "                            Data points:\n",
        "                            [[2.5, 2.4],\n",
        "                            [0.5, 0.7],\n",
        "                            [2.2, 2.9],\n",
        "                            [1.9, 2.2],\n",
        "                            [3.1, 3.0],\n",
        "                            [2.3, 2.7],\n",
        "                            [2.0, 1.6],\n",
        "                            [1.0, 1.1],\n",
        "                            [1.5, 1.6],\n",
        "                            [1.1, 0.9]]\n",
        "\n",
        "                            Step 1: Covariance Matrix\n",
        "\n",
        "                            After centering the data (subtracting the mean), compute the covariance matrix:\n",
        "\n",
        "                            Covariance Matrix:\n",
        "                            [[0.6165, 0.6154],\n",
        "                            [0.6154, 0.7166]]\n",
        "\n",
        "                            Step 2: Compute Eigenvalues and Eigenvectors\n",
        "                            Eigenvalues:      [1.2840, 0.0491]\n",
        "                            Eigenvectors:\n",
        "                            [[ 0.6779, -0.7352],\n",
        "                            [ 0.7352,  0.6779]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                Concept\tRole in PCA\n",
        "\n",
        "                Eigenvectors\tDetermine the directions (principal components) along which data is projected.\n",
        "                Eigenvalues\tTell how much variance (information) each principal component contains.\n",
        "\n",
        "                In our example:\n",
        "\n",
        "                The first principal component (eigenvector [0.6779, 0.7352]) captures ~96% of the total variance (since 1.2840 / (1.2840 + 0.0491) ≈ 0.963).\n",
        "\n",
        "                The second component adds little new information, so we can reduce dimensionality by keeping only the first one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5  How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              PCA reduces the dimensionality of the data, keeping the most important features.\n",
        "\n",
        "              KNN then works more effectively and efficiently in this reduced space.\n",
        "\n",
        "              Pipeline Flow:\n",
        "\n",
        "              Raw Data (High-Dimensional)\n",
        "              ↓\n",
        "\n",
        "              PCA – Dimensionality Reduction\n",
        "              ↓\n",
        "\n",
        "              KNN – Classification or Regression on Reduced Data\n",
        "\n",
        "              \n",
        "              Challenge\tHow PCA Helps\n",
        "              KNN suffers from the curse of dimensionality (distance becomes less meaningful in high dimensions)\tPCA projects data to fewer dimensions while preserving variance\n",
        "              High-dimensional data leads to slow computation in KNN\tPCA reduces the number of features, speeding up distance calculations\n",
        "              Many features may be noisy or irrelevant\tPCA filters out low-variance features, improving KNN accuracy\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "                    Let's say we use the Digits dataset from scikit-learn (1797 samples, 64 features per image):\n",
        "\n",
        "                    from sklearn.datasets import load_digits\n",
        "                    from sklearn.decomposition import PCA\n",
        "                    from sklearn.neighbors import KNeighborsClassifier\n",
        "                    from sklearn.model_selection import train_test_split\n",
        "                    from sklearn.pipeline import Pipeline\n",
        "                    from sklearn.metrics import accuracy_score\n",
        "\n",
        "                    # Load dataset\n",
        "                    X, y = load_digits(return_X_y=True)\n",
        "\n",
        "                    # Train-test split\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                    # Build pipeline: PCA (30 components) + KNN (k=3)\n",
        "                    pipeline = Pipeline([\n",
        "                        ('pca', PCA(n_components=30)),\n",
        "                        ('knn', KNeighborsClassifier(n_neighbors=3))\n",
        "                    ])\n",
        "\n",
        "                    # Train the model\n",
        "                    pipeline.fit(X_train, y_train)\n",
        "\n",
        "                    # Predict and evaluate\n",
        "                    y_pred = pipeline.predict(X_test)\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        Output:\n",
        "        \n",
        "        \n",
        "                  Test Accuracy: 0.9806\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              from sklearn.datasets import load_wine\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.neighbors import KNeighborsClassifier\n",
        "              from sklearn.preprocessing import StandardScaler\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Load the Wine dataset\n",
        "              X, y = load_wine(return_X_y=True)\n",
        "\n",
        "              # Train-test split\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "              # ----------- KNN Without Feature Scaling -----------\n",
        "              knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "              knn_no_scaling.fit(X_train, y_train)\n",
        "              y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "              accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "              # ----------- KNN With Feature Scaling -----------\n",
        "              scaler = StandardScaler()\n",
        "              X_train_scaled = scaler.fit_transform(X_train)\n",
        "              X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "              knn_with_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "              knn_with_scaling.fit(X_train_scaled, y_train)\n",
        "              y_pred_with_scaling = knn_with_scaling.predict(X_test_scaled)\n",
        "              accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "              # Print results\n",
        "              print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "              print(f\"Accuracy with scaling:    {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "\n",
        "        Accuracy without scaling: 0.6944\n",
        "        Accuracy with scaling:    0.9722\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7  Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              from sklearn.datasets import load_wine\n",
        "              from sklearn.preprocessing import StandardScaler\n",
        "              from sklearn.decomposition import PCA\n",
        "\n",
        "              # Step 1: Load the Wine dataset\n",
        "              X, y = load_wine(return_X_y=True)\n",
        "\n",
        "              # Step 2: Standardize the data\n",
        "              scaler = StandardScaler()\n",
        "              X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "              # Step 3: Apply PCA\n",
        "              pca = PCA()\n",
        "              X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "              # Step 4: Print explained variance ratio\n",
        "              explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Output the variance ratio of each principal component\n",
        "            for i, var_ratio in enumerate(explained_variance, start=1):\n",
        "                print(f\"Principal Component {i}: {var_ratio:.4f}\")\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "            Principal Component 1: 0.3619\n",
        "            Principal Component 2: 0.1921\n",
        "            Principal Component 3: 0.1111\n",
        "            Principal Component 4: 0.0704\n",
        "            Principal Component 5: 0.0656\n",
        "            Principal Component 6: 0.0494\n",
        "            Principal Component 7: 0.0419\n",
        "            Principal Component 8: 0.0273\n",
        "            Principal Component 9: 0.0230\n",
        "            Principal Component 10: 0.0189\n",
        "            Principal Component 11: 0.0170\n",
        "            Principal Component 12: 0.0124\n",
        "            Principal Component 13: 0.0089\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8  Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset?\n",
        "\n",
        "\n",
        "\n",
        "                from sklearn.datasets import load_wine\n",
        "                from sklearn.model_selection import train_test_split\n",
        "                from sklearn.neighbors import KNeighborsClassifier\n",
        "                from sklearn.preprocessing import StandardScaler\n",
        "                from sklearn.decomposition import PCA\n",
        "                from sklearn.metrics import accuracy_score\n",
        "\n",
        "                # Load the Wine dataset\n",
        "                X, y = load_wine(return_X_y=True)\n",
        "\n",
        "                # Split into train and test sets\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                # Standardize features for both original and PCA datasets\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                # ----------- KNN on Original Data -----------\n",
        "                knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "                knn_original.fit(X_train_scaled, y_train)\n",
        "                y_pred_original = knn_original.predict(X_test_scaled)\n",
        "                accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "                # ----------- PCA Transformation (Top 2 Components) -----------\n",
        "                pca = PCA(n_components=2)\n",
        "                X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "                X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "                # ----------- KNN on PCA-Transformed Data -----------\n",
        "                knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "                knn_pca.fit(X_train_pca, y_train)\n",
        "                y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "                accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "                # Print accuracies\n",
        "                print(f\"Accuracy on original dataset: {accuracy_original:.4f}\")\n",
        "                print(f\"Accuracy on PCA-transformed dataset (2 components): {accuracy_pca:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "\n",
        "        Accuracy on original dataset: 0.9722\n",
        "        Accuracy on PCA-transformed dataset (2 components): 0.8611\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9  Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            from sklearn.datasets import load_wine\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.neighbors import KNeighborsClassifier\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # Load the Wine dataset\n",
        "            X, y = load_wine(return_X_y=True)\n",
        "\n",
        "            # Train-test split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Scale the features\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            # ----------- KNN with Euclidean distance -----------\n",
        "            knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "            knn_euclidean.fit(X_train_scaled, y_train)\n",
        "            y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "            accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "            # ----------- KNN with Manhattan distance -----------\n",
        "            knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "            knn_manhattan.fit(X_train_scaled, y_train)\n",
        "            y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "            accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "            # Print the accuracies\n",
        "            print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.4f}\")\n",
        "            print(f\"Accuracy with Manhattan distance: {accuracy_manhattan:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "        Accuracy with Euclidean distance: 0.9722\n",
        "        Accuracy with Manhattan distance: 0.9722\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10  You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "      Explain how you would:\n",
        "\n",
        "      Use PCA to reduce dimensionality\n",
        "      Decide how many components to keep\n",
        "      Use KNN for classification post-dimensionality reduction\n",
        "      Evaluate the model\n",
        "      Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "      biomedical data?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                import numpy as np\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.datasets import make_classification\n",
        "                from sklearn.decomposition import PCA\n",
        "                from sklearn.neighbors import KNeighborsClassifier\n",
        "                from sklearn.model_selection import train_test_split, cross_val_score\n",
        "                from sklearn.preprocessing import StandardScaler\n",
        "                from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "                # Simulate a high-dimensional gene expression dataset\n",
        "                X, y = make_classification(n_samples=200, n_features=1000, n_informative=50, n_redundant=0, random_state=42)\n",
        "\n",
        "                # Step 1: Standardize data\n",
        "                scaler = StandardScaler()\n",
        "                X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "                # Step 2: Apply PCA without limiting components to analyze explained variance\n",
        "                pca = PCA()\n",
        "                X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "                # Step 3: Plot cumulative explained variance to decide components to keep\n",
        "                cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n",
        "                plt.plot(cum_var_exp, marker='o')\n",
        "                plt.xlabel('Number of Principal Components')\n",
        "                plt.ylabel('Cumulative Explained Variance')\n",
        "                plt.title('Explained Variance vs Number of Components')\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "                # Choose components to retain 95% variance\n",
        "                n_components = np.argmax(cum_var_exp >= 0.95) + 1\n",
        "                print(f\"Number of components chosen to retain 95% variance: {n_components}\")\n",
        "\n",
        "                # Step 4: Transform data with chosen components\n",
        "                pca = PCA(n_components=n_components)\n",
        "                X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "                # Step 5: Split data for classification\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                # Step 6: Train KNN on reduced data\n",
        "                knn = KNeighborsClassifier(n_neighbors=5)\n",
        "                knn.fit(X_train, y_train)\n",
        "\n",
        "                # Step 7: Predict and evaluate\n",
        "                y_pred = knn.predict(X_test)\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                print(f\"Test Accuracy after PCA + KNN: {accuracy:.4f}\")\n",
        "\n",
        "                # Detailed classification report\n",
        "                print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "                # Optional: cross-validation score for robustness\n",
        "                cv_scores = cross_val_score(knn, X_reduced, y, cv=5)\n",
        "                print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "\n",
        "              Plot of cumulative explained variance showing how many components capture 95% variance\n",
        "\n",
        "              Number of components chosen (e.g.):\n",
        "\n",
        "\n",
        "              Number of components chosen to retain 95% variance: 90\n",
        "              Accuracy on test set:\n",
        "\n",
        "\n",
        "              Test Accuracy after PCA + KNN: 0.8950\n",
        "              Classification report with precision, recall, and F1-score for each class\n",
        "\n",
        "              Cross-validation accuracy:\n",
        "\n",
        "\n",
        "              Cross-Validation Accuracy: 0.8900 ± 0.0300\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yNSlyFfqQRxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qyQplNaFQePB"
      }
    }
  ]
}