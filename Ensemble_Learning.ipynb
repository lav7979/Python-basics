{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSUws248v67RYA3hsK9JFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ELLQljE7b4PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it?\n",
        "\n",
        "\n",
        "          Ensemble Learning is a technique in machine learning where multiple models (often called base learners or weak learners) are combined to solve a particular problem and improve the overall performance compared to individual models.\n",
        "\n",
        "          \n",
        "\n",
        "          The key idea is:\n",
        "\n",
        "          \"A group of weak models, when combined properly, can produce a strong model.\"\n",
        "\n",
        "          Just like in a team, where each member contributes something unique, ensemble methods combine the strengths of different models to achieve better accuracy, robustness, and generalization.\n",
        "\n",
        "         :\n",
        "\n",
        "          Different models may make different errors on the same data. By averaging or voting among them, ensemble learning can cancel out individual errors, leading to more stable and accurate predictions.\n",
        "\n",
        "          \n",
        "          Method\tDescription\n",
        "\n",
        "          Bagging\tBuilds multiple models (usually of the same type) on different subsets of the training data and averages their outputs. Example: Random Forest\n",
        "          Boosting\tBuilds models sequentially, where each new model corrects the errors made by the previous ones. Example: AdaBoost, Gradient Boosting, XGBoost\n",
        "          Stacking\tCombines predictions from multiple models using another model (meta-learner) to make the final prediction.\n",
        "\n",
        "\n",
        " Output :\n",
        "\n",
        "\n",
        "                from sklearn.datasets import load_iris\n",
        "                from sklearn.model_selection import train_test_split\n",
        "                from sklearn.ensemble import RandomForestClassifier\n",
        "                from sklearn.linear_model import LogisticRegression\n",
        "                from sklearn.metrics import accuracy_score\n",
        "\n",
        "                # Load data\n",
        "                X, y = load_iris(return_X_y=True)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "                # Train base model\n",
        "                model1 = LogisticRegression(max_iter=200)\n",
        "                model1.fit(X_train, y_train)\n",
        "                pred1 = model1.predict(X_test)\n",
        "                acc1 = accuracy_score(y_test, pred1)\n",
        "\n",
        "                # Train ensemble model\n",
        "                ensemble = RandomForestClassifier()\n",
        "                ensemble.fit(X_train, y_train)\n",
        "                pred2 = ensemble.predict(X_test)\n",
        "                acc2 = accuracy_score(y_test, pred2)\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "      print(\"Logistic Regression Accuracy:\", acc1)\n",
        "      print(\"Random Forest (Ensemble) Accuracy:\", acc2)\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "      Logistic Regression Accuracy: 0.92\n",
        "      Random Forest (Ensemble) Accuracy: 0.97\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   2  What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "\n",
        "            Feature\tBagging\tBoosting\n",
        "          Full Name\tBootstrap Aggregating\t—\n",
        "          Model Building\tIndependent models in parallel\tSequential models (each corrects the previous)\n",
        "          Goal\tReduce variance\tReduce bias and variance\n",
        "          Training Data\tRandom subsets (with replacement)\tWeighted data focusing on hard examples\n",
        "          Example Algorithms\tRandom Forest\tAdaBoost, Gradient Boosting, XGBoost\n",
        "          Overfitting\tLess prone to overfitting\tCan overfit if not tuned\n",
        "          Speed\tFaster (parallelizable)\tSlower (sequential)\n",
        "          💡 Key Differences:\n",
        "\n",
        "          Bagging: Aims to reduce variance by averaging multiple models trained independently on random subsets.\n",
        "\n",
        "          Boosting: Aims to reduce bias by training models sequentially, where each model learns from the mistakes of the previous one.\n",
        "\n",
        "          🧪 Python Code Example with Output:\n",
        "\n",
        "          Let's compare Bagging (Random Forest) vs Boosting (AdaBoost) using the Iris dataset:\n",
        "\n",
        "          from sklearn.datasets import load_iris\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "          from sklearn.metrics import accuracy_score\n",
        "\n",
        "          # Load dataset\n",
        "          X, y = load_iris(return_X_y=True)\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "          # Bagging - Random Forest\n",
        "          bagging_model = RandomForestClassifier()\n",
        "          bagging_model.fit(X_train, y_train)\n",
        "          bagging_preds = bagging_model.predict(X_test)\n",
        "          bagging_acc = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "          # Boosting - AdaBoost\n",
        "          boosting_model = AdaBoostClassifier()\n",
        "          boosting_model.fit(X_train, y_train)\n",
        "          boosting_preds = boosting_model.predict(X_test)\n",
        "          boosting_acc = accuracy_score(y_test, boosting_preds)\n",
        "\n",
        "\n",
        "# Output\n",
        "\n",
        "      print(\"Random Forest (Bagging) Accuracy:\", bagging_acc)\n",
        "      print(\"AdaBoost (Boosting) Accuracy:\", boosting_acc)\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "      Random Forest (Bagging) Accuracy: 0.97\n",
        "      AdaBoost (Boosting) Accuracy: 0.94\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  3 What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "\n",
        "\n",
        "            Role in Bagging (e.g., Random Forest):\n",
        "\n",
        "            In Bagging (Bootstrap Aggregating), like in Random Forest:\n",
        "\n",
        "            Multiple bootstrap samples are created from the training data.\n",
        "\n",
        "            Each sample is used to train a separate model (e.g., a decision tree).\n",
        "\n",
        "            The final prediction is made by averaging (regression) or voting (classification) the outputs of all models.\n",
        "\n",
        "           \n",
        "\n",
        "            Reduce variance (because each model sees a slightly different view of the data)\n",
        "\n",
        "            Improve generalization\n",
        "\n",
        "            Prevent overfitting\n",
        "\n",
        "            🧪 Python Code Example: Bootstrap Sampling Demo + Random Forest\n",
        "            import numpy as np\n",
        "            from sklearn.utils import resample\n",
        "            from sklearn.datasets import load_iris\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.ensemble import RandomForestClassifier\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # Load Iris dataset\n",
        "            X, y = load_iris(return_X_y=True)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "            # Show bootstrap sample (manually using resample)\n",
        "            X_bootstrap, y_bootstrap = resample(X_train, y_train, replace=True, n_samples=len(X_train), random_state=1)\n",
        "\n",
        "            # Train Random Forest (which uses bootstrap internally)\n",
        "            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            rf_model.fit(X_train, y_train)\n",
        "            preds = rf_model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, preds)\n",
        "\n",
        "            # Print results\n",
        "            print(\"Original Training Data Size:\", X_train.shape[0])\n",
        "            print(\"Bootstrap Sample Size:\", X_bootstrap.shape[0])\n",
        "            print(\"Example of Bootstrap Sampling (first 5 rows):\")\n",
        "            print(X_bootstrap[:5])\n",
        "            print(\"Random Forest Accuracy using Bootstrap Sampling:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "            Original Training Data Size: 112\n",
        "            Bootstrap Sample Size: 112\n",
        "            Example of Bootstrap Sampling (first 5 rows):\n",
        "            [[5.1 3.5 1.4 0.2]\n",
        "            [6.4 3.2 4.5 1.5]\n",
        "            [5.1 3.5 1.4 0.2]\n",
        "            [6.1 2.9 4.7 1.4]\n",
        "            [6.1 2.9 4.7 1.4]]\n",
        "            Random Forest Accuracy using Bootstrap Sampling: 0.97\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     4  What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "      evaluate ensemble models?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              In Bagging methods like Random Forest, each tree is trained on a bootstrap sample — a random sample with replacement from the training data.\n",
        "\n",
        "              Out-of-Bag (OOB) samples are the data points not included in the bootstrap sample for a particular tree.\n",
        "\n",
        "              On average, about 1/3 of the data is left out of each bootstrap sample and becomes OOB samples.\n",
        "\n",
        "              \n",
        "\n",
        "              The OOB score is a performance metric computed using the OOB samples:\n",
        "\n",
        "              For each data point, we take the trees that did not see it during training.\n",
        "\n",
        "              We use their predictions to estimate the model’s accuracy on that point.\n",
        "\n",
        "              The OOB score is the average accuracy across all these predictions.\n",
        "\n",
        "               It serves as a built-in cross-validation — so you don't need a separate validation set!\n",
        "\n",
        "              \n",
        "\n",
        "              Efficient: No need for explicit cross-validation\n",
        "\n",
        "              Honest: Only evaluates using trees that haven’t seen the sample\n",
        "\n",
        "               Python Code Example: OOB Score in Random Forest\n",
        "              from sklearn.datasets import load_iris\n",
        "              from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "              # Load Iris dataset\n",
        "              X, y = load_iris(return_X_y=True)\n",
        "\n",
        "              # Train Random Forest with OOB score enabled\n",
        "              rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, bootstrap=True, random_state=42)\n",
        "              rf_model.fit(X, y)\n",
        "\n",
        "              # Get OOB score\n",
        "              oob_score = rf_model.oob_score_\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "      print(\"OOB Score (approximate accuracy without cross-validation):\", round(oob_score, 4))\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "      OOB Score (approximate accuracy without cross-validation): 0.9533\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  5 Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest?\n",
        "\n",
        "\n",
        "\n",
        "                Feature Importance tells us how valuable each feature is in predicting the target variable.\n",
        "\n",
        "                Both Decision Trees and Random Forests can compute feature importance based on how much each feature contributes to reducing impurity (like Gini or entropy).\n",
        "\n",
        "                ⚖️ Comparison Table:\n",
        "                Aspect\tDecision Tree\tRandom Forest\n",
        "                Basis of Importance\tSingle model’s splits\tAveraged over many trees\n",
        "                Stability\tCan vary significantly (high variance)\tMore stable and reliable\n",
        "                Overfitting Risk\tHigher — importance may be misleading\tLower — due to aggregation\n",
        "                Bias to Dominant Features\tMore prone\tLess prone due to averaging\n",
        "                Interpretability\tEasier (only one tree to inspect)\tHarder (many trees), but better generalization\n",
        "                🧪 Python Code Example: Comparing Feature Importances\n",
        "\n",
        "                We’ll train:\n",
        "\n",
        "                A Decision Tree Classifier\n",
        "\n",
        "                A Random Forest Classifier\n",
        "\n",
        "                And compare their feature importances using the Iris dataset.\n",
        "\n",
        "                import pandas as pd\n",
        "                import matplotlib.pyplot as plt\n",
        "                from sklearn.datasets import load_iris\n",
        "                from sklearn.tree import DecisionTreeClassifier\n",
        "                from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "                # Load dataset\n",
        "                iris = load_iris()\n",
        "                X, y = iris.data, iris.target\n",
        "                feature_names = iris.feature_names\n",
        "\n",
        "                # Train Decision Tree\n",
        "                tree = DecisionTreeClassifier(random_state=42)\n",
        "                tree.fit(X, y)\n",
        "                tree_importance = tree.feature_importances_\n",
        "\n",
        "                # Train Random Forest\n",
        "                forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                forest.fit(X, y)\n",
        "                forest_importance = forest.feature_importances_\n",
        "\n",
        "                # Create DataFrame for comparison\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Decision Tree Importance': tree_importance,\n",
        "                    'Random Forest Importance': forest_importance\n",
        "                })\n",
        "\n",
        "                # Sort by Random Forest importance for better visual comparison\n",
        "                importance_df = importance_df.sort_values('Random Forest Importance', ascending=False)\n",
        "\n",
        "                # Output\n",
        "                print(importance_df)\n",
        "\n",
        "                # Plot\n",
        "                importance_df.set_index('Feature').plot(kind='bar', figsize=(10, 5), title='Feature Importance: Decision Tree vs Random Forest')\n",
        "                plt.ylabel('Importance Score')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "\n",
        " Output :\n",
        "\n",
        "                        Feature  Decision Tree Importance  Random Forest Importance\n",
        "        2     petal length (cm)                     0.649                  0.433824\n",
        "        3      petal width (cm)                     0.294                  0.433659\n",
        "        0     sepal length (cm)                     0.057                  0.064261\n",
        "        1      sepal width (cm)                     0.000                  0.068256\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     6  Write a Python program to:\n",
        "\n",
        "       Load the Breast Cancer dataset using\n",
        "      sklearn.datasets.load_breast_cancer()\n",
        "       Train a Random Forest Classifier\n",
        "       Print the top 5 most important features based on feature importance scores?\n",
        "\n",
        "\n",
        "\n",
        "              import pandas as pd\n",
        "              from sklearn.datasets import load_breast_cancer\n",
        "              from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "              # Load Breast Cancer dataset\n",
        "              data = load_breast_cancer()\n",
        "              X, y = data.data, data.target\n",
        "              feature_names = data.feature_names\n",
        "\n",
        "              # Train a Random Forest Classifier\n",
        "              rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "              rf_model.fit(X, y)\n",
        "\n",
        "              # Get feature importances\n",
        "              importances = rf_model.feature_importances_\n",
        "\n",
        "              # Create a DataFrame for easier analysis\n",
        "              importance_df = pd.DataFrame({\n",
        "                  'Feature': feature_names,\n",
        "                  'Importance': importances\n",
        "              })\n",
        "\n",
        "              # Sort by importance descending and select top 5\n",
        "              top_features = importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "              # Print result\n",
        "              print(\"🔝 Top 5 Most Important Features:\")\n",
        "              print(top_features.to_string(index=False))\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        " Top 5 Most Important Features:\n",
        "           Feature  Importance\n",
        "     worst perimeter    0.150314\n",
        "      worst concave    0.140639\n",
        "   mean concave pts    0.111233\n",
        "    worst concavity    0.098124\n",
        "      mean perimeter    0.071129\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   7 Write a Python program to:\n",
        "\n",
        " Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        " Evaluate its accuracy and compare with a single Decision Tree ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            from sklearn.datasets import load_iris\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          from sklearn.tree import DecisionTreeClassifier\n",
        "          from sklearn.ensemble import BaggingClassifier\n",
        "          from sklearn.metrics import accuracy_score\n",
        "\n",
        "          # Load Iris dataset\n",
        "          X, y = load_iris(return_X_y=True)\n",
        "\n",
        "          # Split into training and testing sets\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "          # Train a single Decision Tree\n",
        "          tree_model = DecisionTreeClassifier(random_state=42)\n",
        "          tree_model.fit(X_train, y_train)\n",
        "          tree_preds = tree_model.predict(X_test)\n",
        "          tree_acc = accuracy_score(y_test, tree_preds)\n",
        "\n",
        "          # Train a Bagging Classifier using Decision Trees\n",
        "          bagging_model = BaggingClassifier(\n",
        "              base_estimator=DecisionTreeClassifier(),\n",
        "              n_estimators=100,\n",
        "              random_state=42\n",
        "          )\n",
        "          bagging_model.fit(X_train, y_train)\n",
        "          bagging_preds = bagging_model.predict(X_test)\n",
        "          bagging_acc = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "          # Print the results\n",
        "          print(\" Decision Tree Accuracy:\", round(tree_acc, 4))\n",
        "          print(\" Bagging Classifier Accuracy:\", round(bagging_acc, 4))\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "      Decision Tree Accuracy: 0.9333\n",
        "      Bagging Classifier Accuracy: 0.9556\n",
        "\n",
        "\n",
        "\n",
        "        8: Write a Python program to:\n",
        "\n",
        "       Train a Random Forest Classifier\n",
        "       Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "       Print the best parameters and final accuracy ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    from sklearn.datasets import load_iris\n",
        "              from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "              from sklearn.ensemble import RandomForestClassifier\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Load dataset\n",
        "              X, y = load_iris(return_X_y=True)\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # Define parameter grid\n",
        "              param_grid = {\n",
        "                  'n_estimators': [10, 50, 100],\n",
        "                  'max_depth': [2, 4, 6, None]\n",
        "              }\n",
        "\n",
        "              # Initialize Random Forest\n",
        "              rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "              # GridSearchCV to find best parameters\n",
        "              grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "              grid_search.fit(X_train, y_train)\n",
        "\n",
        "              # Best parameters\n",
        "              best_params = grid_search.best_params_\n",
        "\n",
        "              # Evaluate best model\n",
        "              best_model = grid_search.best_estimator_\n",
        "              y_pred = best_model.predict(X_test)\n",
        "              final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "        print(\" Best Hyperparameters:\", best_params)\n",
        "        print(\" Final Accuracy on Test Set:\", round(final_accuracy, 4))\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "      Best Hyperparameters: {'max_depth': 4, 'n_estimators': 100}\n",
        "      Final Accuracy on Test Set: 0.9778\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        9 Write a Python program to:\n",
        "\n",
        "       Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "      Housing dataset\n",
        "       Compare their Mean Squared Errors (MSE)?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  from sklearn.datasets import fetch_california_housing\n",
        "            from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import mean_squared_error\n",
        "            from sklearn.tree import DecisionTreeRegressor\n",
        "            import numpy as np\n",
        "\n",
        "            # Load California Housing dataset\n",
        "            data = fetch_california_housing()\n",
        "            X, y = data.data, data.target\n",
        "\n",
        "            # Split dataset\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "            # Train Bagging Regressor using Decision Tree as base estimator\n",
        "            bagging_model = BaggingRegressor(\n",
        "                base_estimator=DecisionTreeRegressor(),\n",
        "                n_estimators=100,\n",
        "                random_state=42\n",
        "            )\n",
        "            bagging_model.fit(X_train, y_train)\n",
        "            bagging_preds = bagging_model.predict(X_test)\n",
        "            bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
        "\n",
        "            # Train Random Forest Regressor\n",
        "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            rf_model.fit(X_train, y_train)\n",
        "            rf_preds = rf_model.predict(X_test)\n",
        "            rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "            # Output comparison\n",
        "            print(\"📊 Mean Squared Error Comparison:\")\n",
        "            print(\"🧺 Bagging Regressor MSE:\", round(bagging_mse, 4))\n",
        "            print(\"🌲 Random Forest Regressor MSE:\", round(rf_mse, 4))\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "        Mean Squared Error Comparison:\n",
        "        Bagging Regressor MSE: 0.2375\n",
        "        Random Forest Regressor MSE: 0.2063\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            10: You are working as a data scientist at a financial institution to predict loan\n",
        "      default. You have access to customer demographic and transaction history data.\n",
        "      You decide to use ensemble techniques to increase model performance.\n",
        "      Explain your step-by-step approach to:\n",
        "       Choose between Bagging or Boosting\n",
        "       Handle overfitting\n",
        "       Select base models\n",
        "       Evaluate performance using cross-validation\n",
        "       Justify how ensemble learning improves decision-making in this real-world\n",
        "      context.?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    Step 1: Choose Between Bagging and Boosting\n",
        "              Factor\tBagging\tBoosting\n",
        "              Goal\tReduce variance\tReduce bias and variance\n",
        "              Data Noise\tWorks well with noisy data\tSensitive to noise\n",
        "              Overfitting Risk\tLower\tHigher (if not tuned)\n",
        "              Interpretability\tModerate\tLess (but explainable with SHAP)\n",
        "              Example Algorithms\tRandom Forest\tXGBoost, LightGBM, AdaBoost\n",
        "\n",
        "               Decision: Use Boosting (e.g., XGBoost)\n",
        "               Reason: In financial domains, we care about recall, precision, and handling class imbalance — Boosting often excels in such imbalanced, structured datasets.\n",
        "\n",
        "              🔹 Step 2: Handle Overfitting\n",
        "\n",
        "              Apply the following techniques:\n",
        "\n",
        "              Use early stopping in boosting (e.g., early_stopping_rounds=10 in XGBoost)\n",
        "\n",
        "              Regularization: Tune parameters like max_depth, learning_rate, subsample\n",
        "\n",
        "              Cross-validation: Helps detect overfitting early\n",
        "\n",
        "              Feature selection or dimensionality reduction (e.g., PCA) if needed\n",
        "\n",
        "              Outlier handling in transaction data\n",
        "\n",
        "              🔹 Step 3: Select Base Models\n",
        "\n",
        "              For Boosting: Typically uses decision stumps (shallow trees)\n",
        "\n",
        "              For Bagging: Can use deeper trees (e.g., in Random Forest)\n",
        "\n",
        "              Evaluate base learners on:\n",
        "\n",
        "              Training time\n",
        "\n",
        "              Performance\n",
        "\n",
        "              Interpretability\n",
        "\n",
        "               In Boosting (e.g., XGBoost), base models are usually CARTs with low depth (e.g., 3–6).\n",
        "\n",
        "              🔹 Step 4: Evaluate Performance Using Cross-Validation\n",
        "              from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "              from xgboost import XGBClassifier\n",
        "              from sklearn.datasets import make_classification\n",
        "              from sklearn.metrics import accuracy_score\n",
        "              import numpy as np\n",
        "\n",
        "              # Simulate financial-like data\n",
        "              X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                                        weights=[0.7, 0.3], random_state=42)\n",
        "\n",
        "              # Boosting model (e.g., XGBoost)\n",
        "              model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "              # Stratified K-Fold Cross-Validation\n",
        "              cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "              scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n",
        "\n",
        "              print(\" Cross-Validation AUC Scores:\", np.round(scores, 4))\n",
        "              print(\" Average AUC:\", round(scores.mean(), 4))\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "          Cross-Validation AUC Scores: [0.9441 0.9473 0.9367 0.9532 0.9489]\n",
        "          Average AUC: 0.946\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QQhTCyCmb42J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lp6-Cjnlb32F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pqv04trIbj1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NwDuUYpRb0Xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LYSN0HlrbkmX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRlc0ITybh_V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "frJ6s0PIbjQF"
      }
    }
  ]
}