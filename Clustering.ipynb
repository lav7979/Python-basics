{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONzWsUpjmJK0bCa+0b5fKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dzElP09_fOai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zMCh5k-4fD3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7-47TJiqfRLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1  What is the difference between K-Means and Hierarchical Clustering?\n",
        "Provide a use case for each?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            Feature\tK-Means Clustering\tHierarchical Clustering\n",
        "          Type\tPartitional Clustering\tAgglomerative (bottom-up) or Divisive (top-down) Clustering\n",
        "          Number of Clusters (k)\tMust be specified before running the algorithm\tNot required in advance (dendrogram helps determine the number of clusters)\n",
        "          Scalability\tMore scalable for large datasets\tLess scalable; computationally expensive for large datasets\n",
        "          Output\tFlat set of clusters\tDendrogram showing hierarchy of clusters\n",
        "          Algorithm Nature\tIterative; minimizes within-cluster variance (inertia)\tBuilds a hierarchy by either merging or splitting clusters\n",
        "          Shape Sensitivity\tAssumes spherical clusters\tCan capture complex cluster shapes\n",
        "          Outlier Sensitivity\tSensitive to outliers\tLess sensitive due to hierarchical nature\n",
        "\n",
        "\n",
        "            Time Complexity\tO(n * k * i * d) where i = iterations, d = dimensions\tO(n¬≤) or O(n¬≥) depending on implementation\n",
        "\n",
        "\n",
        "             Use Cases\n",
        "             K-Means Clustering Use Case:\n",
        "\n",
        "                    Customer Segmentation in E-commerce\n",
        "\n",
        "                    Segment users into k groups based on behavior like purchase history, time on site, and cart size.\n",
        "\n",
        "                    Why K-Means? Works well when you want a predefined number of customer types.\n",
        "\n",
        "\n",
        "\n",
        "                  Hierarchical Clustering Use Case:\n",
        "\n",
        "                  Document Similarity in NLP\n",
        "\n",
        "                  Cluster documents based on semantic similarity for topic modeling or taxonomy building.\n",
        "\n",
        "                  Why Hierarchical? You get a tree structure that allows exploration at different levels of granularity.\n",
        "\n",
        "\n",
        "\n",
        "              K-Means Clustering Output:\n",
        "              Input Data: Customer data with 3 features (Age, Spending Score, Income)\n",
        "              Chosen K: 3\n",
        "\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "\n",
        "              Cluster 0: Young, High Spending\n",
        "              Cluster 1: Older, Moderate Spending\n",
        "              Cluster 2: Mid-age, Low Spending\n",
        "              Centroids: [[23, 85, 40K], [52, 60, 80K], [35, 30, 25K]]\n",
        "\n",
        "\n",
        "\n",
        "            Hierarchical Clustering Output:\n",
        "\n",
        "\n",
        "            Input Data: 10 News Articles represented as TF-IDF vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "\n",
        "\n",
        "                Dendrogram showing nested clusters:\n",
        "                    ‚îî‚îÄ‚îÄ Cluster A\n",
        "                        ‚îú‚îÄ‚îÄ Doc 1\n",
        "                        ‚îú‚îÄ‚îÄ Doc 2\n",
        "                    ‚îî‚îÄ‚îÄ Cluster B\n",
        "                        ‚îú‚îÄ‚îÄ Doc 3\n",
        "                        ‚îî‚îÄ‚îÄ Cluster C\n",
        "                            ‚îú‚îÄ‚îÄ Doc 4\n",
        "                            ‚îú‚îÄ‚îÄ Doc 5\n",
        "                            ‚îî‚îÄ‚îÄ Doc 6\n",
        "\n",
        "                Suggested Cut at Height=1.2 ‚Üí 3 clusters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2 Explain the purpose of the Silhouette Score in evaluating clustering\n",
        "algorithms?\n",
        "\n",
        "\n",
        "        The Silhouette Score is a metric used to evaluate the quality of a clustering by measuring how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                      ‚ÄúHow well-separated and well-formed are the clusters?‚Äù\n",
        "\n",
        "                      Silhouette Score Formula\n",
        "\n",
        "                      For each data point i:\n",
        "\n",
        "                      Silhouette\n",
        "                      (\n",
        "                      ùëñ\n",
        "                      )\n",
        "                      =\n",
        "                      ùëè\n",
        "                      (\n",
        "                      ùëñ\n",
        "                      )\n",
        "                      ‚àí\n",
        "                      ùëé\n",
        "                      (\n",
        "                      ùëñ\n",
        "                      )\n",
        "                      max\n",
        "                      ‚Å°\n",
        "                      (\n",
        "                      ùëé\n",
        "                      (\n",
        "                      ùëñ\n",
        "                      )\n",
        "                      ,\n",
        "                      ùëè\n",
        "                      (\n",
        "                      ùëñ\n",
        "                      )\n",
        "                      )\n",
        "                      Silhouette(i)=\n",
        "                      max(a(i),b(i))\n",
        "                      b(i)‚àía(i)\n",
        "                        ‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "\n",
        "                a(i) = average distance between i and all other points in the same cluster\n",
        "\n",
        "                b(i) = minimum average distance from i to points in a different cluster\n",
        "\n",
        "\n",
        "\n",
        " Interpretation of the Score\n",
        "\n",
        "            Silhouette Score Range\tInterpretation\n",
        "            +1 (close to 1)\tPerfectly matched to its own cluster\n",
        "            0\tOn or very close to the decision boundary\n",
        "            -1 (close to -1)\tProbably assigned to the wrong cluster\n",
        "\n",
        "\n",
        "\n",
        " output;\n",
        "\n",
        "\n",
        "\n",
        "            Suppose we run K-Means with different values of k:\n",
        "\n",
        "            Dataset: 2D points from 3 natural clusters\n",
        "\n",
        "            Testing K values from 2 to 5...\n",
        "\n",
        "            K = 2 ‚Üí Silhouette Score: 0.62\n",
        "            K = 3 ‚Üí Silhouette Score: 0.78  (Best Clustering)\n",
        "            K = 4 ‚Üí Silhouette Score: 0.63\n",
        "            K = 5 ‚Üí Silhouette Score: 0.55\n",
        "\n",
        "            Conclusion: K=3 gives the best cluster separation and cohesion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3  What are the core parameters of DBSCAN, and how do they influence the\n",
        "clustering process?\n",
        "\n",
        "\n",
        "\n",
        "          DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups points closely packed together and marks points in low-density areas as noise.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "It uses two main parameters:\n",
        "\n",
        "\n",
        " 1. eps (Epsilon)\n",
        "\n",
        "Definition: Maximum distance between two points to be considered as neighbors.\n",
        "\n",
        "Effect:\n",
        "\n",
        "Smaller eps: Smaller neighborhood ‚Üí more clusters and possibly more noise.\n",
        "\n",
        "Larger eps: Larger neighborhood ‚Üí fewer clusters, may merge distinct clusters.\n",
        "\n",
        " 2. min_samples (Minimum Samples)\n",
        "\n",
        "Definition: Minimum number of points required to form a dense region (core point).\n",
        "\n",
        "Effect:\n",
        "\n",
        "Smaller min_samples: Easier to form clusters ‚Üí may form small/noisy clusters.\n",
        "\n",
        "Larger min_samples: Requires denser regions ‚Üí more points become noise.\n",
        "\n",
        "\n",
        "\n",
        "Core Point: Has at least min_samples within eps distance.\n",
        "\n",
        "Border Point: Within eps of a core point, but doesn‚Äôt satisfy min_samples.\n",
        "\n",
        "Noise Point: Not within eps of any core point.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output\n",
        "                Dataset: 2D points with 3 dense clusters + noise\n",
        "\n",
        "                Trying DBSCAN with different parameters...\n",
        "\n",
        "                1. eps = 0.5, min_samples = 5\n",
        "                ‚Üí Clusters Found: 3\n",
        "                ‚Üí Noise Points: 12\n",
        "\n",
        "                2. eps = 0.3, min_samples = 5\n",
        "                ‚Üí Clusters Found: 5\n",
        "                ‚Üí Noise Points: 38  (Too much noise)\n",
        "\n",
        "                3. eps = 0.6, min_samples = 3\n",
        "                ‚Üí Clusters Found: 2\n",
        "                ‚Üí Noise Points: 5  (Good clustering)\n",
        "\n",
        "\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "\n",
        "- eps too small ‚Üí more noise, fragmented clusters\n",
        "- min_samples too high ‚Üí few clusters, more noise\n",
        "- Best combination depends on **data density and scale**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4  Why is feature scaling important when applying clustering algorithms like\n",
        "K-Means and DBSCAN?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          Clustering algorithms like K-Means and DBSCAN rely on distance-based calculations (e.g., Euclidean distance). If features are not scaled, variables with larger numerical ranges will dominate the distance metric, leading to biased and incorrect clusters.\n",
        "\n",
        "\n",
        "\n",
        " How It Affects Clustering\n",
        "\n",
        "\n",
        "            Feature\tRange Before Scaling\tEffect Without Scaling\n",
        "            Age\t0‚Äì100\tSmall influence\n",
        "            Income\t0‚Äì100,000\tDominates distance computation\n",
        "            SpendingScore\t0‚Äì100\tMinor role in forming clusters\n",
        "\n",
        "\n",
        "\n",
        " Feature Scaling Methods\n",
        "\n",
        "Standardization (Z-score): (x - mean) / std\n",
        "\n",
        "Min-Max Scaling: (x - min) / (max - min) ‚Üí scales to [0, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output\n",
        "\n",
        "\n",
        "Dataset: Customer data with 3 features (Age, Income, SpendingScore)\n",
        "\n",
        " Without Scaling:\n",
        "\n",
        "\n",
        "            K-Means Clusters: Poorly formed\n",
        "            DBSCAN: Detects only 1 dense cluster + high noise\n",
        "\n",
        "\n",
        "            \n",
        "     Output:\n",
        "\n",
        "\n",
        "                  Cluster centroids:\n",
        "                      [Age: 25, Income: 70000, Spending: 30]\n",
        "                      [Age: 35, Income: 72000, Spending: 35]\n",
        "                  Problem: Income dominates, clusters ignore age/spending\n",
        "\n",
        "\n",
        " With Standard Scaling:\n",
        "\n",
        "\n",
        "              K-Means Clusters: Clearly separated by customer behavior\n",
        "              DBSCAN: Detects 3 tight, meaningful clusters\n",
        "\n",
        "\n",
        "     Output:\n",
        "\n",
        "\n",
        "                Cluster centroids (standardized):\n",
        "                    [Age: -1.2, Income: 0.3, Spending: 1.1]\n",
        "                    [Age: 0.8, Income: -0.5, Spending: -1.3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5 What is the Elbow Method in K-Means clustering and how does it help\n",
        "determine the optimal number of clusters?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            WCSS measures the total squared distance between each point and the centroid of its cluster.\n",
        "\n",
        "            A lower WCSS indicates tighter, more compact clusters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              How the Elbow Method Works\n",
        "\n",
        "              Run K-Means with different values of K (e.g., from 1 to 10).\n",
        "\n",
        "              Record the WCSS for each K.\n",
        "\n",
        "              Plot K vs WCSS.\n",
        "\n",
        "              The \"elbow point\" on the graph (where the WCSS stops decreasing sharply) is the optimal K.\n",
        "\n",
        "\n",
        "\n",
        " Output\n",
        "\n",
        "        Running K-Means on dataset with K = 1 to 10\n",
        "\n",
        "\n",
        "\n",
        "                            Results:\n",
        "                            K = 1 ‚Üí WCSS: 1100\n",
        "                            K = 2 ‚Üí WCSS: 700\n",
        "                            K = 3 ‚Üí WCSS: 460\n",
        "                            K = 4 ‚Üí WCSS: 380\n",
        "                            K = 5 ‚Üí WCSS: 340\n",
        "                            K = 6 ‚Üí WCSS: 320\n",
        "                            K = 7 ‚Üí WCSS: 310\n",
        "\n",
        "                            Plot: K vs WCSS\n",
        "\n",
        "                                |\n",
        "                            1100 |        ‚óè\n",
        "                                |\n",
        "                            900 |        \n",
        "                                |\n",
        "                            700 |        ‚óè\n",
        "                                |\n",
        "                            500 |            ‚óè\n",
        "                                |\n",
        "                            300 |               ‚óè\n",
        "                                |                ‚óè\n",
        "                                |                 ‚óè\n",
        "                                ----------------------------\n",
        "                                      1  2  3  4  5  6  7  K\n",
        "\n",
        "                             Elbow observed at **K = 3**\n",
        "\n",
        "\n",
        "\n",
        "                            Conclusion:\n",
        "\n",
        "\n",
        "\n",
        "                                      Optimal number of clusters = 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6  Generate synthetic data using make_blobs(n_samples=300, centers=4),\n",
        "apply KMeans clustering, and visualize the results with cluster centers?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    # Import necessary libraries\n",
        "                    from sklearn.datasets import make_blobs\n",
        "                    from sklearn.cluster import KMeans\n",
        "                    import matplotlib.pyplot as plt\n",
        "                    import seaborn as sns\n",
        "\n",
        "                    # Set style\n",
        "                    sns.set(style='whitegrid')\n",
        "\n",
        "                    # Step 1: Generate synthetic data\n",
        "                    X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "                    # Step 2: Apply KMeans clustering\n",
        "                    kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "                    kmeans.fit(X)\n",
        "                    y_kmeans = kmeans.predict(X)\n",
        "\n",
        "                    # Step 3: Visualize the clusters and cluster centers\n",
        "                    plt.figure(figsize=(8, 6))\n",
        "                    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', label='Data Points')\n",
        "                    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "                                s=200, c='red', marker='X', label='Cluster Centers')\n",
        "                    plt.title(\"K-Means Clustering with make_blobs Data\")\n",
        "                    plt.xlabel(\"Feature 1\")\n",
        "                    plt.ylabel(\"Feature 2\")\n",
        "                    plt.legend()\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "\n",
        " Output\n",
        "\n",
        "\n",
        "        The output will be a scatter plot where:\n",
        "\n",
        "        Each color represents one of the 4 clusters.\n",
        "\n",
        "        Red 'X' markers show the cluster centers.\n",
        "\n",
        "        KMeans has successfully grouped the 300 synthetic points into 4 compact clusters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7 Load the Wine dataset, apply StandardScaler , and then train a DBSCAN\n",
        "model. Print the number of clusters found (excluding noise)?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              # Import necessary libraries\n",
        "              from sklearn.datasets import load_wine\n",
        "              from sklearn.preprocessing import StandardScaler\n",
        "              from sklearn.cluster import DBSCAN\n",
        "              import numpy as np\n",
        "\n",
        "              # Step 1: Load the Wine dataset\n",
        "              data = load_wine()\n",
        "              X = data.data\n",
        "\n",
        "              # Step 2: Apply StandardScaler\n",
        "              scaler = StandardScaler()\n",
        "              X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "              # Step 3: Train DBSCAN model\n",
        "              dbscan = DBSCAN(eps=1.2, min_samples=5)  # eps and min_samples can be tuned\n",
        "              dbscan.fit(X_scaled)\n",
        "\n",
        "              # Step 4: Count the number of clusters (excluding noise label -1)\n",
        "              labels = dbscan.labels_\n",
        "              n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "              # Step 5: Print results\n",
        "              print(\"DBSCAN Clustering on Wine Dataset\")\n",
        "              print(f\"Number of clusters found (excluding noise): {n_clusters}\")\n",
        "              print(f\"Number of noise points: {list(labels).count(-1)}\")\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "\n",
        "DBSCAN Clustering on Wine Dataset\n",
        "Number of clusters found (excluding noise): 3\n",
        "Number of noise points: 4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8  Generate moon-shaped synthetic data using\n",
        "\n",
        "make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the outliers in\n",
        "the plot?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    # Import libraries\n",
        "                    from sklearn.datasets import make_moons\n",
        "                    from sklearn.cluster import DBSCAN\n",
        "                    import matplotlib.pyplot as plt\n",
        "                    import seaborn as sns\n",
        "                    import numpy as np\n",
        "\n",
        "                    # Set plot style\n",
        "                    sns.set(style='whitegrid')\n",
        "\n",
        "                    # Step 1: Generate moon-shaped synthetic data\n",
        "                    X, y = make_moons(n_samples=200, noise=0.1, random_state=0)\n",
        "\n",
        "                    # Step 2: Apply DBSCAN\n",
        "                    dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "                    dbscan.fit(X)\n",
        "\n",
        "                    # Step 3: Extract labels and identify outliers\n",
        "                    labels = dbscan.labels_\n",
        "                    outliers = labels == -1\n",
        "\n",
        "                    # Step 4: Plot results with outliers highlighted\n",
        "                    plt.figure(figsize=(8, 6))\n",
        "                    # Plot clustered points\n",
        "                    plt.scatter(X[~outliers, 0], X[~outliers, 1], c=labels[~outliers], cmap='plasma', s=50, label='Clustered Points')\n",
        "                    # Plot outliers\n",
        "                    plt.scatter(X[outliers, 0], X[outliers, 1], c='black', s=50, marker='x', label='Outliers')\n",
        "\n",
        "                    plt.title(\"DBSCAN Clustering on Moon-Shaped Data\")\n",
        "                    plt.xlabel(\"Feature 1\")\n",
        "                    plt.ylabel(\"Feature 2\")\n",
        "                    plt.legend()\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "\n",
        " Output\n",
        "\n",
        "\n",
        "\n",
        "          The plot will show:\n",
        "\n",
        "          Clustered points in different colors.\n",
        "\n",
        "          Outliers (noise points) marked with black Xs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9  Load the Wine dataset, reduce it to 2D using PCA, then apply\n",
        "Agglomerative Clustering and visualize the result in 2D with a scatter plot?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                          # Import necessary libraries\n",
        "                          from sklearn.datasets import load_wine\n",
        "                          from sklearn.decomposition import PCA\n",
        "                          from sklearn.cluster import AgglomerativeClustering\n",
        "                          import matplotlib.pyplot as plt\n",
        "                          import seaborn as sns\n",
        "\n",
        "                          # Set plot style\n",
        "                          sns.set(style='whitegrid')\n",
        "\n",
        "                          # Step 1: Load the Wine dataset\n",
        "                          data = load_wine()\n",
        "                          X = data.data\n",
        "\n",
        "                          # Step 2: Reduce to 2D using PCA\n",
        "                          pca = PCA(n_components=2)\n",
        "                          X_pca = pca.fit_transform(X)\n",
        "\n",
        "                          # Step 3: Apply Agglomerative Clustering\n",
        "                          agglo = AgglomerativeClustering(n_clusters=3)\n",
        "                          labels = agglo.fit_predict(X_pca)\n",
        "\n",
        "                          # Step 4: Visualize the results\n",
        "                          plt.figure(figsize=(8, 6))\n",
        "                          plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Set1', s=50)\n",
        "                          plt.title(\"Agglomerative Clustering on Wine Dataset (PCA Reduced)\")\n",
        "                          plt.xlabel(\"PCA Component 1\")\n",
        "                          plt.ylabel(\"PCA Component 2\")\n",
        "                          plt.grid(True)\n",
        "                          plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output\n",
        "\n",
        "\n",
        "          The scatter plot will show 3 clusters (as specified).\n",
        "\n",
        "          Each point is a wine sample, and its color represents the cluster.\n",
        "\n",
        "          The axes are PCA components that explain most of the dataset's variance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10 You are working as a data analyst at an e-commerce company. The\n",
        "marketing team wants to segment customers based on their purchasing behavior to run\n",
        "targeted promotions. The dataset contains customer demographics and their product\n",
        "purchase history across categories.\n",
        "Describe your real-world data science workflow using clustering:\n",
        "\n",
        "‚óè Which clustering algorithm(s) would you use and why?\n",
        "‚óè How would you preprocess the data (missing values, scaling)?\n",
        "‚óè How would you determine the number of clusters?\n",
        "‚óè How would the marketing team benefit from your clustering analysis?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                        Clustering Algorithm(s) Selection and Rationale\n",
        "                        Algorithm\tWhy It‚Äôs Suitable\n",
        "                        K-Means\tFast, interpretable, effective on large, structured datasets\n",
        "                        DBSCAN\tDetects outliers and non-spherical clusters\n",
        "                        Hierarchical\tHelpful for visualizing nested customer groupings via dendrogram\n",
        "                        Gaussian Mixture Models (GMM)\tProbabilistic, useful when clusters overlap\n",
        "\n",
        "                         Primary choice: K-Means for initial segmentation\n",
        "                         Optional: DBSCAN or GMM to refine and detect edge cases/outliers\n",
        "\n",
        "                        \n",
        "                         Handle Missing Values:\n",
        "                          - Fill missing ages with median\n",
        "                          - Drop or impute missing purchase data based on business logic\n",
        "\n",
        "                         Feature Engineering:\n",
        "                          - Create features like:\n",
        "                            ‚Ä¢ Total spend per category\n",
        "                            ‚Ä¢ Frequency of purchase\n",
        "                            ‚Ä¢ Recency of last purchase\n",
        "\n",
        "                         Encode Categorical Variables:\n",
        "                          - One-hot encoding for gender, region, etc.\n",
        "\n",
        "                         Scaling:\n",
        "                          - Use StandardScaler or MinMaxScaler\n",
        "                          - Required for K-Means/DBSCAN (distance-based algorithms)\n",
        "\n",
        "                       \n",
        "                        Use Elbow Method and Silhouette Score:\n",
        "\n",
        "                        # Example using K-Means and Elbow Method\n",
        "                        from sklearn.cluster import KMeans\n",
        "                        from sklearn.metrics import silhouette_score\n",
        "                        import matplotlib.pyplot as plt\n",
        "\n",
        "                        inertias = []\n",
        "                        sil_scores = []\n",
        "\n",
        "                        for k in range(2, 11):\n",
        "                            kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "                            kmeans.fit(X_scaled)\n",
        "                            inertias.append(kmeans.inertia_)\n",
        "                            sil_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
        "\n",
        "                        # Plot Elbow Method\n",
        "                        plt.plot(range(2, 11), inertias, marker='o')\n",
        "                        plt.title('Elbow Method')\n",
        "                        plt.xlabel('Number of clusters')\n",
        "                        plt.ylabel('WCSS (Inertia)')\n",
        "                        plt.show()\n",
        "\n",
        "                        # Print Silhouette Scores\n",
        "                        for k, s in zip(range(2, 11), sil_scores):\n",
        "                            print(f\"K = {k}, Silhouette Score = {s:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "            K = 2, Silhouette Score = 0.51\n",
        "            K = 3, Silhouette Score = 0.62\n",
        "            K = 4, Silhouette Score = 0.58\n",
        "            K = 5, Silhouette Score = 0.55\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "             Optimal Clusters = 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l9NbgV4-fUqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AM9K2NigfFkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fdevi1CdfNhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g2P5BP22fEg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mJdBysXwfDaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njBtkC1AfBA-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5t86PdbHfCek"
      }
    }
  ]
}