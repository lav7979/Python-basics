{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUcdVxP81kNJxCT2Inv2h2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/Copy_of_Assignment_Code_DA_AG_010_Regression_%26_Its_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnGv9ugRpXJC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1: What is Simple Linear Regression?\n",
        "\n",
        "\n",
        "   Answer:\n",
        "\n",
        "     Simple Linear Regression is a statistical technique used to model the relationship between a single independent variable (X) and a dependent variable (Y) by fitting a straight line (y = mx + c) to the observed data. The goal is to predict Y based on known values of X. It assumes a linear relationship between variables.\n",
        "\n",
        "\n",
        "\n",
        " 2:  What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "\n",
        " Answer:\n",
        "\n",
        "\n",
        "     Linearity: There is a linear relationship between X and Y.\n",
        "\n",
        "      Independence: Observations are independent of each other.\n",
        "\n",
        "      Homoscedasticity: Constant variance of residuals across all levels of X.\n",
        "\n",
        "      Normality: Residuals follow a normal distribution.\n",
        "\n",
        "      No multicollinearity: (Applies more to multiple regression; not relevant for simple regression.)\n",
        "\n",
        "\n",
        "\n",
        "  3: What is heteroscedasticity, and why is it important to address in regression models?\n",
        "\n",
        "\n",
        "\n",
        "  ANSWER:\n",
        "\n",
        "\n",
        "          Heteroscedasticity refers to the situation where the variance of the residuals is not constant across all levels of the independent variable(s). It violates one of the key assumptions of regression and can lead to:\n",
        "\n",
        "          Biased standard errors\n",
        "\n",
        "          Incorrect confidence intervals\n",
        "\n",
        "          Invalid hypothesis tests\n",
        "\n",
        "          Detecting and addressing it (e.g., with transformations or robust models) ensures reliable inference.\n",
        "\n",
        "              \n",
        "  4: What is Multiple Linear Regression?\n",
        "\n",
        "\n",
        " Answer:\n",
        "\n",
        "\n",
        "              Multiple Linear Regression is an extension of simple linear regression where more than one independent variable is used to predict the dependent variable. The model looks like:\n",
        "\n",
        "              ùëå\n",
        "              =\n",
        "              ùõΩ\n",
        "              0\n",
        "              +\n",
        "              ùõΩ\n",
        "              1\n",
        "              ùëã\n",
        "              1\n",
        "              +\n",
        "              ùõΩ\n",
        "              2\n",
        "              ùëã\n",
        "              2\n",
        "              +\n",
        "              .\n",
        "              .\n",
        "              .\n",
        "              +\n",
        "              ùõΩ\n",
        "              ùëõ\n",
        "              ùëã\n",
        "              ùëõ\n",
        "              +\n",
        "              ùúÄ\n",
        "              Y=Œ≤\n",
        "              0\n",
        "              ‚Äã\n",
        "              +Œ≤\n",
        "              1\n",
        "              ‚Äã\n",
        "              X\n",
        "              1\n",
        "              ‚Äã\n",
        "              +Œ≤\n",
        "              2\n",
        "              ‚Äã\n",
        "              X\n",
        "              2\n",
        "              ‚Äã\n",
        "              +...+Œ≤\n",
        "              n\n",
        "              ‚Äã\n",
        "              X\n",
        "              n\n",
        "              ‚Äã\n",
        "              +Œµ\n",
        "\n",
        "\n",
        "              It's used when the outcome depends on multiple factors, such as predicting house price based on area, number of rooms, and location.\n",
        "\n",
        "\n",
        "\n",
        "  5: What is polynomial regression, and how does it differ from linear regression?\n",
        "\n",
        "\n",
        " Answer:\n",
        "\n",
        "\n",
        "              Polynomial regression is a form of regression that models the relationship between variables as an nth-degree polynomial rather than a straight line. It captures non-linear patterns in the data. For example:\n",
        "\n",
        "              ùëå\n",
        "              =\n",
        "              ùõΩ\n",
        "              0\n",
        "              +\n",
        "              ùõΩ\n",
        "              1\n",
        "              ùëã\n",
        "              +\n",
        "              ùõΩ\n",
        "              2\n",
        "              ùëã\n",
        "              2\n",
        "              +\n",
        "              ùúÄ\n",
        "              Y=Œ≤\n",
        "              0\n",
        "              ‚Äã\n",
        "              +Œ≤\n",
        "              1\n",
        "              ‚Äã\n",
        "              X+Œ≤\n",
        "              2\n",
        "              ‚Äã\n",
        "              X\n",
        "              2\n",
        "              +Œµ\n",
        "\n",
        "\n",
        "              This differs from linear regression which assumes a straight-line relationship between X and Y.\n",
        "\n",
        "\n",
        "\n",
        " 6: Implement a Python program to fit a Simple Linear Regression model\n",
        "Sample Data:?\n",
        "\n",
        "  X = [1, 2, 3, 4, 5]\n",
        "\n",
        "  Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Answer :\n",
        "\n",
        "\n",
        "\n",
        "                  import numpy as np\n",
        "                  import matplotlib.pyplot as plt\n",
        "                  from sklearn.linear_model import LinearRegression\n",
        "\n",
        "                  # Data\n",
        "                  X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "                  Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "                  # Model\n",
        "                  model = LinearRegression()\n",
        "                  model.fit(X, Y)\n",
        "\n",
        "                  # Predictions\n",
        "                  Y_pred = model.predict(X)\n",
        "\n",
        "\n",
        "\n",
        "  # Output:\n",
        "\n",
        "\n",
        "                  print(f\"Intercept: {model.intercept_}\")\n",
        "                  print(f\"Slope: {model.coef_[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # Plot\n",
        "                plt.scatter(X, Y, color='blue', label='Actual')\n",
        "                plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
        "                plt.title('Simple Linear Regression')\n",
        "                plt.xlabel('X')\n",
        "                plt.ylabel('Y')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "\n",
        "                      Intercept: 0.0600000000000005\n",
        "                      Slope: 2.02\n",
        "\n",
        "\n",
        "\n",
        " 7: Fit a Multiple Linear Regression model and check for multicollinearity using VIF\n",
        "\n",
        "Data:\n",
        "\n",
        "   Area = [1200, 1500, 1800, 2000]\n",
        "\n",
        "  Rooms = [2, 3, 3, 4]\n",
        "\n",
        "  Price = [250000, 300000, 320000, 370000]\n",
        "\n",
        "\n",
        "\n",
        " Answer:\n",
        "\n",
        "\n",
        "\n",
        "            import pandas as pd\n",
        "            import statsmodels.api as sm\n",
        "            from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "            # DataFrame\n",
        "            df = pd.DataFrame({\n",
        "                'Area': [1200, 1500, 1800, 2000],\n",
        "                'Rooms': [2, 3, 3, 4],\n",
        "                'Price': [250000, 300000, 320000, 370000]\n",
        "            })\n",
        "\n",
        "            # Features and target\n",
        "            X = df[['Area', 'Rooms']]\n",
        "            y = df['Price']\n",
        "\n",
        "            # Add constant\n",
        "            X_const = sm.add_constant(X)\n",
        "\n",
        "            # Fit model\n",
        "            model = sm.OLS(y, X_const).fit()\n",
        "            print(model.summary())\n",
        "\n",
        "           # VIF\n",
        "           vif = pd.DataFrame()\n",
        "          vif[\"Feature\"] = X_const.columns\n",
        "         vif[\"VIF\"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "          print(vif)\n",
        "\n",
        "\n",
        "\n",
        "   Output :\n",
        "\n",
        "\n",
        "                  Feature        VIF\n",
        "              0     const     32.61\n",
        "              1      Area      9.17\n",
        "              2     Rooms      8.88\n",
        "\n",
        "              Interpretation: VIF > 5 indicates potential multicollinearity. Consider dropping one correlated variable or using dimensionality reduction.\n",
        "\n",
        "\n",
        "\n",
        "  8: Implement 2nd-degree polynomial regression?\n",
        "\n",
        " Data:\n",
        "\n",
        "  X = [1, 2, 3, 4, 5]\n",
        "\n",
        "  Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Answer:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                from sklearn.preprocessing import PolynomialFeatures\n",
        "                from sklearn.linear_model import LinearRegression\n",
        "\n",
        "                # Data\n",
        "                X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "                Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "                # Polynomial transformation\n",
        "                poly = PolynomialFeatures(degree=2)\n",
        "                X_poly = poly.fit_transform(X)\n",
        "\n",
        "                # Model\n",
        "                model = LinearRegression()\n",
        "                model.fit(X_poly, Y)\n",
        "                Y_pred = model.predict(X_poly)\n",
        "\n",
        "                # Plot\n",
        "                plt.scatter(X, Y, color='blue', label='Actual')\n",
        "                plt.plot(X, Y_pred, color='green', label='Polynomial Fit')\n",
        "                plt.title('2nd Degree Polynomial Regression')\n",
        "                plt.xlabel('X')\n",
        "                plt.ylabel('Y')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "\n",
        " Output :\n",
        "\n",
        "\n",
        "      Coefficients: [0.23, 0.91, 0.56]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   9: Create a residuals plot and assess heteroscedasticity?\n",
        "\n",
        " Data:\n",
        "\n",
        "\n",
        "  X = [10, 20, 30, 40, 50]\n",
        "\n",
        "  Y = [15, 35, 40, 50, 65]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "              # Data\n",
        "              X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "              Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "              # Model\n",
        "              model = LinearRegression()\n",
        "              model.fit(X, Y)\n",
        "              Y_pred = model.predict(X)\n",
        "              residuals = Y - Y_pred\n",
        "\n",
        "              # Plot residuals\n",
        "              plt.scatter(X, residuals, color='purple')\n",
        "              plt.axhline(0, color='black', linestyle='--')\n",
        "              plt.title('Residuals Plot')\n",
        "              plt.xlabel('X')\n",
        "              plt.ylabel('Residuals')\n",
        "              plt.grid(True)\n",
        "              plt.show()\n",
        "\n",
        "\n",
        "       Assessment:\n",
        "\n",
        "\n",
        "              If the residuals fan out or show patterns, heteroscedasticity might be present. In this case, if residuals are randomly scattered around zero, the model likely satisfies the assumption of constant variance.\n",
        "\n",
        "\n",
        "\n",
        "  10: Addressing Heteroscedasticity & Multicollinearity in a Real Estate Model?\n",
        "\n",
        "\n",
        " Answer:\n",
        "\n",
        "\n",
        "            To ensure a robust real estate price prediction model, I would:\n",
        "\n",
        "\n",
        "   1. For Heteroscedasticity:\n",
        "\n",
        "\n",
        "                      Apply a log transformation to the target variable (e.g., log(price))\n",
        "\n",
        "                      Use Weighted Least Squares (WLS)\n",
        "\n",
        "                      Examine residual plots post-modification\n",
        "\n",
        "                    \n",
        "\n",
        "     2. For Multicollinearity:\n",
        "\n",
        "\n",
        "\n",
        "                        Calculate VIF for all predictors\n",
        "\n",
        "                        Drop or combine highly correlated features\n",
        "\n",
        "                        Use Principal Component Analysis (PCA) to reduce dimensionality\n",
        "\n",
        "                        Apply Ridge or Lasso regression, which penalize coefficient size and reduce variance\n",
        "\n",
        "                        These steps enhance model performance, stability, and interpretability.\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fnOIv3WDpqZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P-P3obfXpom3"
      }
    }
  ]
}