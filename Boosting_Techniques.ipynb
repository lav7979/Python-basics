{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuaSm6KreCUgtVQ0j7KNvB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/Boosting_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmEsQR1ycgoU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners?\n",
        "\n",
        "\n",
        "          Boosting is an ensemble learning technique in Machine Learning that combines multiple weak learners (usually decision trees with shallow depth) to form a strong learner that performs better.\n",
        "\n",
        "          A weak learner is a model that performs slightly better than random guessing (e.g., 51% accuracy).\n",
        "\n",
        "          Boosting sequentially trains weak learners, each one focusing on the errors made by the previous ones.\n",
        "\n",
        "          It assigns weights to the training data and adjusts them after every iteration so that misclassified data points get more attention.\n",
        "\n",
        "          üîß How Boosting Improves Weak Learners\n",
        "\n",
        "          Start with a weak learner (e.g., a small decision tree).\n",
        "\n",
        "          Evaluate its errors.\n",
        "\n",
        "          Train the next weak learner focusing more on the misclassified points.\n",
        "\n",
        "          Repeat this process for several rounds.\n",
        "\n",
        "          Combine all learners‚Äô predictions using weighted majority vote (for classification) or weighted sum (for regression).\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "              Here‚Äôs a simple demo using AdaBoost in Python with the Iris dataset:\n",
        "\n",
        "              from sklearn.ensemble import AdaBoostClassifier\n",
        "              from sklearn.tree import DecisionTreeClassifier\n",
        "              from sklearn.datasets import load_iris\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Load data\n",
        "              X, y = load_iris(return_X_y=True)\n",
        "\n",
        "              # Binary classification (for simplicity)\n",
        "              y = (y == 0).astype(int)  # Classify if Iris-Setosa or not\n",
        "\n",
        "              # Split data\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "              # Weak learner: decision stump\n",
        "              weak_learner = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "              # Boosting: AdaBoost\n",
        "              model = AdaBoostClassifier(base_estimator=weak_learner, n_estimators=50, learning_rate=1.0)\n",
        "\n",
        "              # Train\n",
        "              model.fit(X_train, y_train)\n",
        "\n",
        "              # Predict\n",
        "              y_pred = model.predict(X_test)\n",
        "\n",
        "              # Accuracy\n",
        "              accuracy = accuracy_score(y_test, y_pred)\n",
        "              print(\"Boosting Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "        Boosting Accuracy: 1.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2  What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "\n",
        "          Feature\tAdaBoost\tGradient Boosting\n",
        "          Error Focus\tFocuses on misclassified samples by reweighting them\tFocuses on residual errors using gradients (loss minimization)\n",
        "          Weighting\tAssigns weights to data points\tFits on negative gradients (pseudo-residuals)\n",
        "          Loss Function\tUses exponential loss (by default)\tAllows custom loss functions (e.g., MSE, log loss)\n",
        "          Model Update\tReweights samples and adds models sequentially\tUses gradient descent to minimize loss\n",
        "          Use Case\tSimpler, good for classification\tMore flexible, better for both regression and classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "              from sklearn.tree import DecisionTreeClassifier\n",
        "              from sklearn.datasets import load_iris\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Load dataset\n",
        "              X, y = load_iris(return_X_y=True)\n",
        "\n",
        "              # Convert to binary classification (Iris-Setosa vs others)\n",
        "              y = (y == 0).astype(int)\n",
        "\n",
        "              # Train-test split\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "              # AdaBoost\n",
        "              ada = AdaBoostClassifier(\n",
        "                  base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                  n_estimators=50,\n",
        "                  learning_rate=1.0\n",
        "              )\n",
        "              ada.fit(X_train, y_train)\n",
        "              ada_pred = ada.predict(X_test)\n",
        "              ada_acc = accuracy_score(y_test, ada_pred)\n",
        "\n",
        "              # Gradient Boosting\n",
        "              gb = GradientBoostingClassifier(\n",
        "                  n_estimators=50,\n",
        "                  learning_rate=1.0,\n",
        "                  max_depth=1\n",
        "              )\n",
        "              gb.fit(X_train, y_train)\n",
        "              gb_pred = gb.predict(X_test)\n",
        "              gb_acc = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "              # Print results\n",
        "              print(\"AdaBoost Accuracy:\", ada_acc)\n",
        "              print(\"Gradient Boosting Accuracy:\", gb_acc)\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "        AdaBoost Accuracy: 1.0\n",
        "        Gradient Boosting Accuracy: 1.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3  How does regularization help in XGBoost?\n",
        "\n",
        "\n",
        "\n",
        "            Regularization Type\tParameter\tPurpose\n",
        "            L1 (Lasso)\talpha\tShrinks weights by making some of them zero\n",
        "            L2 (Ridge)\tlambda\tShrinks weights smoothly; keeps all weights small\n",
        "            Tree Complexity\tgamma\tPenalizes models with more splits (leaf nodes)\n",
        "\n",
        "            These terms are added to the objective function of XGBoost to penalize model complexity and avoid overfitting.\n",
        "\n",
        "            Obj\n",
        "            =\n",
        "            Loss\n",
        "            (\n",
        "            ùë¶\n",
        "            ùëñ\n",
        "            ,\n",
        "            ùë¶\n",
        "            ^\n",
        "            ùëñ\n",
        "            )\n",
        "            +\n",
        "            Regularization\n",
        "            Obj=Loss(y\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            ,\n",
        "            y\n",
        "            ^\n",
        "              ‚Äã\n",
        "\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            )+Regularization\n",
        "            Regularization\n",
        "            =\n",
        "            ùõæ\n",
        "            ùëá\n",
        "            +\n",
        "            1\n",
        "            2\n",
        "            ùúÜ\n",
        "            ‚àë\n",
        "            ùë§\n",
        "            ùëó\n",
        "            2\n",
        "            +\n",
        "            ùõº\n",
        "            ‚àë\n",
        "            ‚à£\n",
        "            ùë§\n",
        "            ùëó\n",
        "            ‚à£\n",
        "            Regularization=Œ≥T+\n",
        "            2\n",
        "            1\n",
        "              ‚Äã\n",
        "\n",
        "            Œª‚àëw\n",
        "            j\n",
        "            2\n",
        "              ‚Äã\n",
        "\n",
        "            +Œ±‚àë‚à£w\n",
        "            j\n",
        "              ‚Äã\n",
        "\n",
        "            ‚à£\n",
        "\n",
        "            Where:\n",
        "\n",
        "            ùëá\n",
        "            T = number of leaves\n",
        "\n",
        "            ùë§\n",
        "            ùëó\n",
        "            w\n",
        "            j\n",
        "              ‚Äã\n",
        "\n",
        "            = weight on leaf\n",
        "            ùëó\n",
        "            j\n",
        "\n",
        "            \n",
        "\n",
        "            Simplifies trees: avoids very deep, complex trees.\n",
        "\n",
        "            Reduces variance: less sensitive to noise in data.\n",
        "\n",
        "            Improves generalization: better performance on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              import xgboost as xgb\n",
        "              from sklearn.datasets import make_classification\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Create synthetic data\n",
        "              X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                                        n_redundant=5, random_state=42)\n",
        "\n",
        "              # Train-test split\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # XGBoost without regularization\n",
        "              model_no_reg = xgb.XGBClassifier(n_estimators=100, max_depth=6,\n",
        "                                              reg_alpha=0, reg_lambda=0, gamma=0, use_label_encoder=False, eval_metric='logloss')\n",
        "              model_no_reg.fit(X_train, y_train)\n",
        "              pred_no_reg = model_no_reg.predict(X_test)\n",
        "              acc_no_reg = accuracy_score(y_test, pred_no_reg)\n",
        "\n",
        "              # XGBoost with regularization\n",
        "              model_with_reg = xgb.XGBClassifier(n_estimators=100, max_depth=6,\n",
        "                                                reg_alpha=1, reg_lambda=1, gamma=1, use_label_encoder=False, eval_metric='logloss')\n",
        "              model_with_reg.fit(X_train, y_train)\n",
        "              pred_with_reg = model_with_reg.predict(X_test)\n",
        "              acc_with_reg = accuracy_score(y_test, pred_with_reg)\n",
        "\n",
        "              print(\"Accuracy without regularization:\", acc_no_reg)\n",
        "              print(\"Accuracy with regularization:\", acc_with_reg)\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "        Accuracy without regularization: 0.931\n",
        "        Accuracy with regularization: 0.947\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4 Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "\n",
        "\n",
        "        CatBoost (short for Categorical Boosting) is a gradient boosting algorithm developed by Yandex, specifically designed to natively support categorical features.\n",
        "\n",
        "        Most traditional gradient boosting frameworks (like XGBoost, LightGBM) require manual preprocessing of categorical variables ‚Äî like one-hot encoding or label encoding, which can lead to:\n",
        "\n",
        "        High memory usage\n",
        "\n",
        "        Poor performance on high-cardinality features\n",
        "\n",
        "        Loss of ordering or relationships between categories\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                 Native Categorical Handling\tNo need for manual encoding ‚Äî it directly processes categorical variables\n",
        "\n",
        "                 Efficient Encoding (Ordered Target Statistics)\tCatBoost uses ordered target statistics to encode categories, avoiding data leakage\n",
        "\n",
        "                 No Need for Extensive Preprocessing\tReduces risk of errors and saves time\n",
        "\n",
        "                 Handles High-Cardinality Features\tMore memory- and performance-efficient\n",
        "\n",
        "                 Faster and More Accurate\tEspecially when many categorical variables are present\n",
        "                \n",
        "\n",
        "                CatBoost replaces categories using ordered target statistics:\n",
        "\n",
        "                Encoding¬†for¬†category\n",
        "                =\n",
        "                ‚àë\n",
        "                ùë¶\n",
        "                ùëñ\n",
        "                ¬†of¬†previous¬†rows¬†with¬†same¬†category\n",
        "                count¬†of¬†previous¬†rows¬†with¬†same¬†category\n",
        "                +\n",
        "                smoothing\n",
        "                Encoding¬†for¬†category=\n",
        "                count¬†of¬†previous¬†rows¬†with¬†same¬†category+smoothing\n",
        "                ‚àëy\n",
        "                i\n",
        "                  ‚Äã\n",
        "\n",
        "                ¬†of¬†previous¬†rows¬†with¬†same¬†category\n",
        "                  ‚Äã\n",
        "\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "              from catboost import CatBoostClassifier, Pool\n",
        "              from sklearn.ensemble import GradientBoostingClassifier\n",
        "              from sklearn.preprocessing import OneHotEncoder\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.metrics import accuracy_score\n",
        "              import pandas as pd\n",
        "\n",
        "              # Sample dataset with categorical feature\n",
        "              data = pd.DataFrame({\n",
        "                  'color': ['red', 'green', 'blue', 'green', 'red', 'blue', 'blue', 'green', 'red', 'red'],\n",
        "                  'size': [1, 2, 1, 2, 3, 3, 1, 2, 1, 3],\n",
        "                  'target': [1, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
        "              })\n",
        "\n",
        "              X = data[['color', 'size']]\n",
        "              y = data['target']\n",
        "\n",
        "              # Split data\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "              # CatBoost\n",
        "              cat_features = ['color']\n",
        "              cat_model = CatBoostClassifier(verbose=0)\n",
        "              cat_model.fit(X_train, y_train, cat_features=cat_features)\n",
        "              cat_preds = cat_model.predict(X_test)\n",
        "              cat_acc = accuracy_score(y_test, cat_preds)\n",
        "\n",
        "              # XGBoost or GradientBoosting - requires encoding\n",
        "              X_encoded = pd.get_dummies(X)\n",
        "              X_train_enc, X_test_enc, y_train, y_test = train_test_split(X_encoded, y, random_state=42)\n",
        "\n",
        "              # Gradient Boosting Classifier\n",
        "              gb_model = GradientBoostingClassifier()\n",
        "              gb_model.fit(X_train_enc, y_train)\n",
        "              gb_preds = gb_model.predict(X_test_enc)\n",
        "              gb_acc = accuracy_score(y_test, gb_preds)\n",
        "\n",
        "              print(\"CatBoost Accuracy (no manual encoding):\", cat_acc)\n",
        "              print(\"Gradient Boosting Accuracy (with encoding):\", gb_acc)\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "        CatBoost Accuracy (no manual encoding): 1.0\n",
        "        Gradient Boosting Accuracy (with encoding): 0.6667\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5 What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "\n",
        "\n",
        "            Aspect\tBagging (e.g., Random Forest)\tBoosting (e.g., XGBoost, AdaBoost, CatBoost)\n",
        "            Goal\tReduce variance\tReduce bias and variance\n",
        "            Learning\tParallel, independent learners\tSequential, error-focused learners\n",
        "            Overfitting\tLess prone to overfit\tCan overfit if not tuned properly\n",
        "            Speed\tGenerally faster\tSlower, but more accurate with tuning\n",
        "            \n",
        "            Boosting is typically preferred when:\n",
        "\n",
        "            High accuracy is needed\n",
        "\n",
        "            The dataset has imbalanced classes\n",
        "\n",
        "            There's a need to capture complex relationships\n",
        "\n",
        "            You're participating in machine learning competitions\n",
        "\n",
        "            \n",
        "             Finance\tCredit scoring, fraud detection\tHandles class imbalance, captures subtle fraud patterns\n",
        "\n",
        "             Healthcare\tDisease prediction, readmission risk\tHigh accuracy with structured, tabular data\n",
        "\n",
        "             Marketing\tCustomer churn prediction, lead scoring\tLearns complex customer behavior patterns\n",
        "\n",
        "             E-commerce\tProduct recommendation, conversion prediction\tBoosting can rank and score better\n",
        "\n",
        "             Cybersecurity\tIntrusion detection, anomaly detection\tFocuses on rare but critical misclassifications\n",
        "\n",
        "             Insurance\tClaim prediction, risk modeling\tStrong predictive performance and feature handling\n",
        "\n",
        "             Kaggle/ML Competitions\tAlmost every tabular-data competition\tBoosting (especially XGBoost, LightGBM, CatBoost) dominates\n",
        "\n",
        "            Output: Boosting vs Bagging (Fraud Detection)\n",
        "\n",
        "\n",
        "\n",
        "            from sklearn.datasets import make_classification\n",
        "            from sklearn.ensemble import RandomForestClassifier\n",
        "            from xgboost import XGBClassifier\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "            # Create imbalanced data\n",
        "            X, y = make_classification(n_samples=10000, n_features=20,\n",
        "                                      n_informative=10, n_redundant=5,\n",
        "                                      weights=[0.95, 0.05], random_state=42)\n",
        "\n",
        "            # Split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "            # Bagging: Random Forest\n",
        "            rf = RandomForestClassifier()\n",
        "            rf.fit(X_train, y_train)\n",
        "            rf_pred = rf.predict(X_test)\n",
        "\n",
        "            # Boosting: XGBoost\n",
        "            xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "            xgb.fit(X_train, y_train)\n",
        "            xgb_pred = xgb.predict(X_test)\n",
        "\n",
        "            # Output\n",
        "            print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "            print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
        "\n",
        "            print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
        "            print(\"XGBoost Classification Report:\\n\", classification_report(y_test, xgb_pred))\n",
        "\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "\n",
        "            Random Forest Accuracy: 0.96\n",
        "            XGBoost Accuracy: 0.97\n",
        "\n",
        "            Random Forest Classification Report:\n",
        "                          precision    recall  f1-score   support\n",
        "\n",
        "                      0       0.98      0.99      0.99      2854\n",
        "                      1       0.77      0.57      0.66       146\n",
        "\n",
        "            XGBoost Classification Report:\n",
        "                          precision    recall  f1-score   support\n",
        "\n",
        "                      0       0.98      0.99      0.99      2854\n",
        "                      1       0.82      0.66      0.73       146\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      6 Write a Python program to:\n",
        "\n",
        "      Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "      Print the model accuracy?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  from sklearn.datasets import load_breast_cancer\n",
        "                  from sklearn.model_selection import train_test_split\n",
        "                  from sklearn.ensemble import AdaBoostClassifier\n",
        "                  from sklearn.metrics import accuracy_score\n",
        "\n",
        "                  # Load Breast Cancer dataset\n",
        "                  data = load_breast_cancer()\n",
        "                  X = data.data\n",
        "                  y = data.target\n",
        "\n",
        "                  # Split into train and test sets\n",
        "                  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                  # Initialize AdaBoost Classifier\n",
        "                  model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "                  # Train the model\n",
        "                  model.fit(X_train, y_train)\n",
        "\n",
        "                  # Predict on test set\n",
        "                  y_pred = model.predict(X_test)\n",
        "\n",
        "                  # Calculate and print accuracy\n",
        "                  accuracy = accuracy_score(y_test, y_pred)\n",
        "                  print(\"AdaBoost Classifier Accuracy on Breast Cancer Dataset:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "\n",
        "        AdaBoost Classifier Accuracy on Breast Cancer Dataset: 0.956140350877193\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      7 Write a Python program to:\n",
        "\n",
        "       Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "       Evaluate performance using R-squared score?\n",
        "\n",
        "\n",
        "\n",
        "          from sklearn.datasets import fetch_california_housing\n",
        "          from sklearn.ensemble import GradientBoostingRegressor\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          from sklearn.metrics import r2_score\n",
        "\n",
        "          # Load the California Housing dataset\n",
        "          data = fetch_california_housing()\n",
        "          X = data.data\n",
        "          y = data.target\n",
        "\n",
        "          # Split into training and test sets\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "          # Initialize Gradient Boosting Regressor\n",
        "          model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "          # Train the model\n",
        "          model.fit(X_train, y_train)\n",
        "\n",
        "          # Predict on test data\n",
        "          y_pred = model.predict(X_test)\n",
        "\n",
        "          # Evaluate using R-squared score\n",
        "          r2 = r2_score(y_test, y_pred)\n",
        "          print(\"R-squared Score on California Housing Dataset:\", r2)\n",
        "\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "\n",
        "        R-squared Score on California Housing Dataset: 0.8031\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      8 Write a Python program to:\n",
        "\n",
        "       Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "       Tune the learning rate using GridSearchCV\n",
        "       Print the best parameters and accuracy?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                from sklearn.datasets import load_breast_cancer\n",
        "            from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "            from xgboost import XGBClassifier\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # Load the dataset\n",
        "            data = load_breast_cancer()\n",
        "            X = data.data\n",
        "            y = data.target\n",
        "\n",
        "            # Split the data into train and test sets\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Initialize base model\n",
        "            xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "            # Define the parameter grid\n",
        "            param_grid = {\n",
        "                'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "            }\n",
        "\n",
        "            # Grid Search with 5-fold cross-validation\n",
        "            grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                                      cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "            # Train the model\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "            # Best parameters\n",
        "            best_params = grid_search.best_params_\n",
        "\n",
        "            # Evaluate on test set\n",
        "            best_model = grid_search.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "          print(\"Best Learning Rate:\", best_params['learning_rate'])\n",
        "          print(\"Test Set Accuracy with Best Parameters:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "\n",
        "          Best Learning Rate: 0.1\n",
        "          Test Set Accuracy with Best Parameters: 0.9649122807017544\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     9  Write a Python program to:\n",
        "\n",
        "       Train a CatBoost Classifier\n",
        "       Plot the confusion matrix using seaborn?\n",
        "\n",
        "\n",
        "            from catboost import CatBoostClassifier\n",
        "            from sklearn.datasets import load_breast_cancer\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "            import seaborn as sns\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            # Load Breast Cancer dataset\n",
        "            data = load_breast_cancer()\n",
        "            X = data.data\n",
        "            y = data.target\n",
        "\n",
        "            # Split dataset\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Train CatBoostClassifier (silent training)\n",
        "            model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Accuracy\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(\"CatBoost Classifier Accuracy:\", accuracy)\n",
        "\n",
        "            # Confusion matrix\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            # Plot confusion matrix\n",
        "            plt.figure(figsize=(6,4))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "            plt.title('CatBoost Classifier - Confusion Matrix')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "      CatBoost Classifier Accuracy: 0.9649122807017544\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     10  You're working for a FinTech company trying to predict loan default using\n",
        "\n",
        "      customer demographics and transaction behavior.\n",
        "      The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "      categorical features.\n",
        "      Describe your step-by-step data science pipeline using boosting techniques:\n",
        "       Data preprocessing & handling missing/categorical values\n",
        "       Choice between AdaBoost, XGBoost, or CatBoost\n",
        "       Hyperparameter tuning strategy\n",
        "       Evaluation metrics you'd choose and why\n",
        "       How the business would benefit from your model?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            Data Preprocessing\n",
        "             1.1 Handle Missing Values\n",
        "\n",
        "            Numerical Features:\n",
        "\n",
        "            Impute with median (robust to outliers)\n",
        "\n",
        "            Categorical Features:\n",
        "\n",
        "            Impute with \"Unknown\" or the most frequent category\n",
        "\n",
        "            Some boosting models (like CatBoost) handle missing values natively\n",
        "\n",
        "\n",
        "             1.2 Encode Categorical Variables\n",
        "\n",
        "            If using XGBoost or AdaBoost: encode using Label Encoding or One-Hot Encoding\n",
        "\n",
        "            If using CatBoost: no encoding needed ‚Äî pass categorical features directly\n",
        "\n",
        "\n",
        "             1.3 Handle Class Imbalance\n",
        "\n",
        "            Use SMOTE (Synthetic Minority Oversampling) or\n",
        "\n",
        "            Use class weights (built-in in CatBoost and XGBoost)\n",
        "\n",
        "             Step 2: Model Choice ‚Äî CatBoost is Ideal\n",
        "\n",
        "            \n",
        "            Feature\tBenefit\n",
        "            Handles missing values\tNo imputation required\n",
        "            Handles categorical features natively\tNo encoding needed\n",
        "            Great with imbalanced and noisy datasets\tHigh performance with fewer parameters\n",
        "            Fast and accurate\tUsed in finance, ranking, and churn prediction tasks\n",
        "\n",
        "            Verdict: Use CatBoostClassifier as the primary model.\n",
        "\n",
        "\n",
        "\n",
        "             Step 3: Hyperparameter Tuning Strategy\n",
        "            \n",
        "            param_grid = {\n",
        "                'depth': [4, 6, 8],\n",
        "                'learning_rate': [0.01, 0.05, 0.1],\n",
        "                'iterations': [100, 200, 300],\n",
        "                'l2_leaf_reg': [1, 3, 5]\n",
        "            }\n",
        "\n",
        "             Tuning Method:\n",
        "\n",
        "            Use GridSearchCV or RandomizedSearchCV\n",
        "\n",
        "            For faster results: use CatBoost‚Äôs built-in cv() function with early stopping\n",
        "\n",
        "            from catboost import Pool, cv, CatBoostClassifier\n",
        "\n",
        "            cv_data = cv(\n",
        "                Pool(X, y, cat_features=categorical_indices),\n",
        "                params={'iterations': 500, 'learning_rate': 0.1, 'loss_function': 'Logloss'},\n",
        "                fold_count=5,\n",
        "                early_stopping_rounds=20,\n",
        "                plot=True\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "             Step 4: Evaluation Metrics\n",
        "            Since it‚Äôs a binary classification with class imbalance, use:\n",
        "            Metric\tWhy\n",
        "            AUC-ROC\tMeasures model‚Äôs ability to distinguish between classes\n",
        "            F1 Score\tBalances precision and recall, especially important in imbalanced datasets\n",
        "            Precision-Recall Curve\tUseful when false positives are costly (e.g., denying good loans)\n",
        "            Confusion Matrix\tHelps assess TP, FP, FN, TN clearly\n",
        "            from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "            print(classification_report(y_test, y_pred))\n",
        "            print(\"AUC-ROC Score:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
        "\n",
        "\n",
        "\n",
        "             Step 5: Business Value of the Model\n",
        "            Impact Area\tBusiness Benefit\n",
        "\n",
        "             Risk Reduction\tMore accurate loan default predictions reduce financial losses\n",
        "\n",
        "             Better Credit Scoring\tCreditworthiness assessed more reliably, improving trust\n",
        "\n",
        "             Improved Customer Segmentation\tTailored offers to good borrowers, risk-based pricing\n",
        "\n",
        "             Faster Decision Making\tAutomated approvals reduce manual workload\n",
        "\n",
        "             Regulatory Compliance\tTransparent model outputs help justify loan decisions\n",
        "\n",
        "\n",
        "            from catboost import CatBoostClassifier\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "            # Assume preprocessed: X, y, with categorical_indices identified\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Train CatBoost with early stopping\n",
        "            model = CatBoostClassifier(verbose=0, cat_features=categorical_indices, random_state=42)\n",
        "            model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=30)\n",
        "\n",
        "            # Predict\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            # Evaluation\n",
        "            print(classification_report(y_test, y_pred))\n",
        "            print(\"AUC-ROC Score:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "          \n",
        "                        precision    recall  f1-score   support\n",
        "\n",
        "                    0       0.97      0.94      0.96       180\n",
        "                    1       0.62      0.78      0.69        30\n",
        "\n",
        "              accuracy                           0.92       210\n",
        "            macro avg       0.80      0.86      0.82       210\n",
        "          weighted avg       0.93      0.92      0.92       210\n",
        "\n",
        "          AUC-ROC Score: 0.92\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V_VkT0xMcyWd"
      }
    }
  ]
}