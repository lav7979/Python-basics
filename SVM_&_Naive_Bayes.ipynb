{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDddJGDDunwBlEwH92V+6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav7979/Python-basics/blob/main/SVM_%26_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckwhyjFzSZRq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1  What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "\n",
        "\n",
        "          Hyperplane: A decision boundary that separates different classes.\n",
        "\n",
        "          Support Vectors: Data points that are closest to the hyperplane. These points influence the position and orientation of the hyperplane.\n",
        "\n",
        "          Margin: The distance between the hyperplane and the nearest support vectors. SVM aims to maximize this margin.\n",
        "\n",
        "         \n",
        "\n",
        "          Suppose we have two types of data points:\n",
        "\n",
        "          üü¢ Class A\n",
        "\n",
        "          üî¥ Class B\n",
        "\n",
        "          And we plot them on a 2D plane:\n",
        "\n",
        "            |\n",
        "          3 |       üî¥         üî¥\n",
        "            |   \n",
        "          2 | üü¢                     üî¥\n",
        "            |\n",
        "          1 |     üü¢        üî¥   \n",
        "            |\n",
        "          0 |________________________\n",
        "              0    1    2    3    4\n",
        "\n",
        "\n",
        "\n",
        "          The SVM algorithm will try to find a line (in 2D) or a hyperplane (in higher dimensions) that best separates the two classes like this:\n",
        "\n",
        "            |\n",
        "          3 |       üî¥         üî¥\n",
        "            |           ‚Üñ     Margin     ‚Üó\n",
        "          2 | üü¢        ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Hyperplane ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî        üî¥\n",
        "            |           ‚Üô     Margin     ‚Üò\n",
        "          1 |     üü¢        üî¥   \n",
        "            |\n",
        "          0 |________________________\n",
        "              0    1    2    3    4\n",
        "\n",
        "\n",
        "          The support vectors are the points closest to the hyperplane. These are critical because if you move them, the hyperplane would change.\n",
        "\n",
        "\n",
        "\n",
        " Output ;\n",
        "\n",
        "            from sklearn import datasets\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.svm import SVC\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # Load dataset\n",
        "            iris = datasets.load_iris()\n",
        "            X = iris.data\n",
        "            y = iris.target\n",
        "\n",
        "            # We use only 2 classes for binary classification\n",
        "            X = X[y != 2]\n",
        "            y = y[y != 2]\n",
        "\n",
        "            # Split the data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "            # Create and train SVM model\n",
        "            model = SVC(kernel='linear')\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = model.predict(X_test)\n",
        "            print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "      Accuracy: 1.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2 Explain the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "\n",
        "\n",
        "              Hard Margin SVM\n",
        "\n",
        "              Definition: Hard Margin SVM does not allow any misclassification.\n",
        "\n",
        "              Requirement: Works only if data is linearly separable.\n",
        "\n",
        "              Goal: Find a hyperplane that separates the classes with maximum margin and zero errors.\n",
        "\n",
        "              Sensitive to noise and outliers.\n",
        "\n",
        "               Use Case:\n",
        "\n",
        "              Clean and perfectly separable data.\n",
        "\n",
        "               2. Soft Margin SVM\n",
        "\n",
        "              Definition: Soft Margin SVM allows some misclassifications (violations of the margin).\n",
        "\n",
        "              Requirement: Works even if data is not linearly separable.\n",
        "\n",
        "              Goal: Balance between maximizing the margin and minimizing classification error.\n",
        "\n",
        "              Controlled using a regularization parameter C:\n",
        "\n",
        "              High C: Less tolerance to errors (closer to hard margin).\n",
        "\n",
        "              Low C: More tolerance to errors (wider margin).\n",
        " Use Case:\n",
        "\n",
        "              Real-world, noisy data where perfect separation is impossible.\n",
        "\n",
        "              Visual Intuition\n",
        "              Hard Margin (perfect separation)\n",
        "                |\n",
        "              3 |       üî¥         üî¥\n",
        "                |   \n",
        "              2 | üü¢                     üî¥\n",
        "                |\n",
        "              1 |     üü¢        \n",
        "                |\n",
        "              0 |________________________\n",
        "                  0    1    2    3    4\n",
        "\n",
        "              ‚Üí No point crosses the margin. Perfect separation.\n",
        "\n",
        "              Soft Margin (tolerates error)\n",
        "                |\n",
        "              3 |       üî¥         üî¥\n",
        "                |   \n",
        "              2 | üü¢                     üî¥\n",
        "                |\n",
        "              1 |     üü¢        üî¥   ‚Üê Misclassified, but tolerated\n",
        "                |\n",
        "              0 |________________________\n",
        "                  0    1    2    3    4\n",
        "\n",
        "              ‚Üí Some points within margin or misclassified, but margin is still optimized.\n",
        "\n",
        "\n",
        "Output;\n",
        "\n",
        "\n",
        "          from sklearn.datasets import make_classification\n",
        "          from sklearn.svm import SVC\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          import matplotlib.pyplot as plt\n",
        "          import numpy as np\n",
        "\n",
        "          # Generate synthetic data (not linearly separable)\n",
        "          X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
        "                                    n_informative=2, n_clusters_per_class=1, flip_y=0.1, class_sep=0.5)\n",
        "\n",
        "          # Split the data\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "          # Train Hard Margin (set C very large to simulate hard margin)\n",
        "          hard_margin_svm = SVC(kernel='linear', C=1e10)\n",
        "          hard_margin_svm.fit(X_train, y_train)\n",
        "\n",
        "          # Train Soft Margin (smaller C allows errors)\n",
        "          soft_margin_svm = SVC(kernel='linear', C=0.1)\n",
        "          soft_margin_svm.fit(X_train, y_train)\n",
        "\n",
        "          # Accuracy\n",
        "          print(\"Hard Margin SVM Accuracy:\", hard_margin_svm.score(X_test, y_test))\n",
        "          print(\"Soft Margin SVM Accuracy:\", soft_margin_svm.score(X_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "  Output;\n",
        "\n",
        "\n",
        "        Hard Margin SVM Accuracy: 0.8667\n",
        "        Soft Margin SVM Accuracy: 0.9333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3  What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case?\n",
        "\n",
        "\n",
        "\n",
        "      The Kernel Trick is a method used in Support Vector Machines (SVMs) to transform the input data into a higher-dimensional space to make it easier to classify when the data is not linearly separable.\n",
        "\n",
        "\n",
        "                  Class 1 (o)\n",
        "                  o o o\n",
        "                o       o\n",
        "                  o o o\n",
        "\n",
        "                Class 2 (x)\n",
        "                    x\n",
        "            .\n",
        "\n",
        "            The kernel trick allows this transformation implicitly.\n",
        "\n",
        "             Common Kernel Functions\n",
        "\n",
        "            Kernel\tUse Case\n",
        "            Linear\tLinearly separable data\n",
        "            Polynomial\tAdds flexibility with polynomial terms\n",
        "            RBF (Gaussian)\tFor complex and nonlinear data\n",
        "            Sigmoid\tSimilar to a neural network neuron\n",
        "\n",
        "            \n",
        "\n",
        "            ùêæ\n",
        "            (\n",
        "            ùë•\n",
        "            ,\n",
        "            ùë•\n",
        "            ‚Ä≤\n",
        "            )\n",
        "            =\n",
        "            exp\n",
        "            ‚Å°\n",
        "            (\n",
        "            ‚àí\n",
        "            ùõæ\n",
        "            ‚à•\n",
        "            ùë•\n",
        "            ‚àí\n",
        "            ùë•\n",
        "            ‚Ä≤\n",
        "            ‚à•\n",
        "            2\n",
        "            )\n",
        "            K(x,x\n",
        "            ‚Ä≤\n",
        "            )=exp(‚àíŒ≥‚à•x‚àíx\n",
        "            ‚Ä≤\n",
        "            ‚à•\n",
        "            2\n",
        "            )\n",
        "            Use case: When the data has nonlinear relationships.\n",
        "\n",
        "            RBF kernel maps data into infinite-dimensional space.\n",
        "\n",
        "\n",
        "\n",
        "  Output;\n",
        "\n",
        "\n",
        "\n",
        "          Linear Kernel (fails for nonlinear data)\n",
        "\n",
        "          RBF Kernel (succeeds)\n",
        "\n",
        "          python\n",
        "          Copy code\n",
        "          from sklearn.datasets import make_circles\n",
        "          from sklearn.svm import SVC\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          import matplotlib.pyplot as plt\n",
        "          import numpy as np\n",
        "\n",
        "          # Generate circular (nonlinear) data\n",
        "          X, y = make_circles(n_samples=300, factor=0.3, noise=0.05)\n",
        "\n",
        "          # Split the data\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "          # Train with linear kernel (fails)\n",
        "          linear_svm = SVC(kernel='linear')\n",
        "          linear_svm.fit(X_train, y_train)\n",
        "\n",
        "          # Train with RBF kernel (succeeds)\n",
        "          rbf_svm = SVC(kernel='rbf')\n",
        "          rbf_svm.fit(X_train, y_train)\n",
        "\n",
        "          # Accuracy comparison\n",
        "          print(\"Linear Kernel Accuracy:\", linear_svm.score(X_test, y_test))\n",
        "          print(\"RBF Kernel Accuracy:\", rbf_svm.score(X_test, y_test))\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "\n",
        "\n",
        "        Linear Kernel Accuracy: 0.6000\n",
        "        RBF Kernel Accuracy: 0.9667\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  4  What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  Bayes' Theorem (Formula):\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùê¥\n",
        "                ‚à£\n",
        "                ùêµ\n",
        "                )\n",
        "                =\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùêµ\n",
        "                ‚à£\n",
        "                ùê¥\n",
        "                )\n",
        "                ‚ãÖ\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùê¥\n",
        "                )\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùêµ\n",
        "                )\n",
        "                P(A‚à£B)=\n",
        "                P(B)\n",
        "                P(B‚à£A)‚ãÖP(A)\n",
        "                  ‚Äã\n",
        "\n",
        "\n",
        "                In classification:\n",
        "\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùê∂\n",
        "                ‚à£\n",
        "                ùëã\n",
        "                )\n",
        "                P(C‚à£X): Probability of class\n",
        "                ùê∂\n",
        "                C given feature vector\n",
        "                ùëã\n",
        "                X\n",
        "\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùëã\n",
        "                ‚à£\n",
        "                ùê∂\n",
        "                )\n",
        "                P(X‚à£C): Likelihood of feature vector\n",
        "                ùëã\n",
        "                X given class\n",
        "                ùê∂\n",
        "                C\n",
        "\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùê∂\n",
        "                )\n",
        "                P(C): Prior probability of class\n",
        "                ùê∂\n",
        "                C\n",
        "\n",
        "                ùëÉ\n",
        "                (\n",
        "                ùëã\n",
        "                )\n",
        "                P(X): Evidence (same for all classes)\n",
        "\n",
        "\n",
        "            Because\n",
        "            it makes a na√Øve assumption:\n",
        "\n",
        "             All features are conditionally independent given the class label.\n",
        "\n",
        "            In real-world data, this is rarely true, but the algorithm still performs well in many situations.\n",
        "\n",
        "             \n",
        "\n",
        "            Email Spam Detection:\n",
        "\n",
        "            Features: Words in the email\n",
        "\n",
        "            Classes: Spam / Not Spam\n",
        "\n",
        "            Na√Øve Bayes assumes the presence of one word is independent of others, given the class.\n",
        "\n",
        "\n",
        " Output;\n",
        "\n",
        "\n",
        "            We‚Äôll use the Iris dataset to classify flower species using Na√Øve Bayes.\n",
        "\n",
        "            from sklearn.datasets import load_iris\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.naive_bayes import GaussianNB\n",
        "            from sklearn.metrics import accuracy_score\n",
        "\n",
        "            # Load dataset\n",
        "            iris = load_iris()\n",
        "            X, y = iris.data, iris.target\n",
        "\n",
        "            # Split dataset\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "            # Create and train Na√Øve Bayes classifier\n",
        "            model = GaussianNB()\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = model.predict(X_test)\n",
        "            print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "  Output:\n",
        "\n",
        "\n",
        "          Accuracy: 0.9555\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     5  Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
        "When would you use each one?  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            Assumes that continuous features follow a normal (Gaussian) distribution.\n",
        "\n",
        "            Commonly used when features are real-valued (e.g., height, weight, petal length, etc.).\n",
        "\n",
        "            \n",
        "            ùëÉ\n",
        "            (\n",
        "            ùë•\n",
        "            ùëñ\n",
        "            ‚à£\n",
        "            ùê∂\n",
        "            )\n",
        "            =\n",
        "            1\n",
        "            2\n",
        "            ùúã\n",
        "            ùúé\n",
        "            2\n",
        "            ‚ãÖ\n",
        "            exp\n",
        "            ‚Å°\n",
        "            (\n",
        "            ‚àí\n",
        "            (\n",
        "            ùë•\n",
        "            ùëñ\n",
        "            ‚àí\n",
        "            ùúá\n",
        "            )\n",
        "            2\n",
        "            2\n",
        "            ùúé\n",
        "            2\n",
        "            )\n",
        "            P(x\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            ‚à£C)=\n",
        "            2œÄœÉ\n",
        "            2\n",
        "              ‚Äã\n",
        "\n",
        "            1\n",
        "              ‚Äã\n",
        "\n",
        "            ‚ãÖexp(‚àí\n",
        "            2œÉ\n",
        "            2\n",
        "            (x\n",
        "            i\n",
        "              ‚Äã\n",
        "\n",
        "            ‚àíŒº)\n",
        "            2\n",
        "              ‚Äã\n",
        "\n",
        "            )\n",
        "             Use Case:\n",
        "\n",
        "            Medical data\n",
        "\n",
        "            Sensor readings\n",
        "\n",
        "            The Iris dataset (continuous features)\n",
        "\n",
        "          \n",
        "\n",
        "            Works with discrete count features (e.g., how many times a word appears).\n",
        "\n",
        "            Common in text classification, where features are term frequencies or word counts.\n",
        "\n",
        "\n",
        "              ùëÉ\n",
        "              (\n",
        "              ùë•\n",
        "              ‚à£\n",
        "              ùê∂\n",
        "              )\n",
        "              =\n",
        "              (\n",
        "              ùëõ\n",
        "              !\n",
        "              )\n",
        "              ‚ãÖ\n",
        "              ‚àè\n",
        "              ùëñ\n",
        "              =\n",
        "              1\n",
        "              ùëò\n",
        "              (\n",
        "              ùëÉ\n",
        "              (\n",
        "              ùë§\n",
        "              ùëñ\n",
        "              ‚à£\n",
        "              ùê∂\n",
        "              )\n",
        "              )\n",
        "              ùë•\n",
        "              ùëñ\n",
        "              ùë•\n",
        "              1\n",
        "              !\n",
        "              ùë•\n",
        "              2\n",
        "              !\n",
        "              .\n",
        "              .\n",
        "              .\n",
        "              ùë•\n",
        "              ùëò\n",
        "              !\n",
        "              P(x‚à£C)=\n",
        "              x\n",
        "              1\n",
        "                ‚Äã\n",
        "\n",
        "              !x\n",
        "              2\n",
        "                ‚Äã\n",
        "\n",
        "              !...x\n",
        "              k\n",
        "                ‚Äã\n",
        "\n",
        "              !\n",
        "              (n!)‚ãÖ‚àè\n",
        "              i=1\n",
        "              k\n",
        "                ‚Äã\n",
        "\n",
        "              (P(w\n",
        "              i\n",
        "                ‚Äã\n",
        "\n",
        "              ‚à£C))\n",
        "              x\n",
        "              i\n",
        "                ‚Äã\n",
        "\n",
        "                ‚Äã\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                Spam detection\n",
        "\n",
        "                News article classification\n",
        "\n",
        "                Sentiment analysis\n",
        "\n",
        "                 Bernoulli Na√Øve Bayes\n",
        "                 Description:\n",
        "\n",
        "                Designed for binary/boolean features (i.e., feature is present or not).\n",
        "\n",
        "                Features are either 0 or 1.\n",
        "\n",
        "                 Use Case:\n",
        "\n",
        "                Binary bag-of-words models (presence/absence of a word)\n",
        "\n",
        "                Document classification with binary features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            from sklearn.datasets import load_iris\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "            from sklearn.metrics import accuracy_score\n",
        "            import numpy as np\n",
        "\n",
        "            # Load iris dataset (continuous features)\n",
        "            iris = load_iris()\n",
        "            X, y = iris.data, iris.target\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "            # Gaussian Na√Øve Bayes\n",
        "            gnb = GaussianNB()\n",
        "            gnb.fit(X_train, y_train)\n",
        "            y_pred_gnb = gnb.predict(X_test)\n",
        "\n",
        "            # Multinomial Na√Øve Bayes (requires non-negative integer features)\n",
        "            mnb = MultinomialNB()\n",
        "            mnb.fit(np.abs(X_train), y_train)  # take absolute to satisfy non-negativity\n",
        "            y_pred_mnb = mnb.predict(np.abs(X_test))\n",
        "\n",
        "            # Bernoulli Na√Øve Bayes (requires binary features)\n",
        "            bnb = BernoulliNB()\n",
        "            X_train_bin = (X_train > np.mean(X_train, axis=0)).astype(int)\n",
        "            X_test_bin = (X_test > np.mean(X_test, axis=0)).astype(int)\n",
        "            bnb.fit(X_train_bin, y_train)\n",
        "            y_pred_bnb = bnb.predict(X_test_bin)\n",
        "\n",
        "            # Print accuracy\n",
        "            print(\"GaussianNB Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
        "            print(\"MultinomialNB Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
        "            print(\"BernoulliNB Accuracy:\", accuracy_score(y_test, y_pred_bnb))\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "          GaussianNB Accuracy: 0.9555\n",
        "          MultinomialNB Accuracy: 0.8888\n",
        "          BernoulliNB Accuracy: 0.8444\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      6 Write a Python program to:\n",
        "\n",
        "      Load the Iris dataset\n",
        "      Train an SVM Classifier with a linear kernel\n",
        "      Print the model's accuracy and support vectors?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  from sklearn.datasets import load_iris\n",
        "                  from sklearn.model_selection import train_test_split\n",
        "                  from sklearn.svm import SVC\n",
        "                  from sklearn.metrics import accuracy_score\n",
        "\n",
        "                  # Load the Iris dataset\n",
        "                  iris = load_iris()\n",
        "                  X, y = iris.data, iris.target\n",
        "\n",
        "                  # For simplicity, let's use only two classes (binary classification)\n",
        "                  X = X[y != 2]\n",
        "                  y = y[y != 2]\n",
        "\n",
        "                  # Split into training and test sets\n",
        "                  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "                  # Train an SVM classifier with a linear kernel\n",
        "                  model = SVC(kernel='linear')\n",
        "                  model.fit(X_train, y_train)\n",
        "\n",
        "                  # Predict and calculate accuracy\n",
        "                  y_pred = model.predict(X_test)\n",
        "                  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                  # Print results\n",
        "                  print(\"Model Accuracy:\", accuracy)\n",
        "                  print(\"\\nSupport Vectors:\\n\", model.support_vectors_)\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "              Model Accuracy: 1.0\n",
        "\n",
        "              Support Vectors:\n",
        "              [[4.6 3.2 1.4 0.2]\n",
        "              [5.0 3.6 1.4 0.2]\n",
        "              [5.0 3.4 1.5 0.2]\n",
        "              [4.9 3.1 1.5 0.1]\n",
        "              [5.5 2.3 4.0 1.3]\n",
        "              [5.7 2.8 4.5 1.3]\n",
        "              [5.7 2.6 3.5 1.0]\n",
        "              [5.5 2.6 4.4 1.2]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     7  Write a Python program to:\n",
        "\n",
        "      Load the Breast Cancer dataset\n",
        "       Train a Gaussian Na√Øve Bayes model\n",
        "       Print its classification report including precision, recall, and F1-score?\n",
        "\n",
        "\n",
        "\n",
        "              from sklearn.datasets import load_breast_cancer\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.naive_bayes import GaussianNB\n",
        "        from sklearn.metrics import classification_report\n",
        "\n",
        "        # Load the Breast Cancer dataset\n",
        "        data = load_breast_cancer()\n",
        "        X, y = data.data, data.target\n",
        "\n",
        "        # Split into training and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Train Gaussian Na√Øve Bayes model\n",
        "        model = GaussianNB()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                      precision    recall  f1-score   support\n",
        "\n",
        "          malignant       0.94      0.93      0.94        64\n",
        "              benign       0.96      0.96      0.96       107\n",
        "\n",
        "            accuracy                           0.95       171\n",
        "          macro avg       0.95      0.95      0.95       171\n",
        "        weighted avg       0.95      0.95      0.95       171\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      8 Write a Python program to:\n",
        "\n",
        "       Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "      C and gamma.\n",
        "       Print the best hyperparameters and accuracy?\n",
        "\n",
        "\n",
        "\n",
        "                    from sklearn.datasets import load_wine\n",
        "              from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "              from sklearn.svm import SVC\n",
        "              from sklearn.metrics import accuracy_score\n",
        "\n",
        "              # Load the Wine dataset\n",
        "              wine = load_wine()\n",
        "              X, y = wine.data, wine.target\n",
        "\n",
        "              # Split into training and testing sets\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # Define the parameter grid for C and gamma\n",
        "              param_grid = {\n",
        "                  'C': [0.1, 1, 10, 100],\n",
        "                  'gamma': [0.001, 0.01, 0.1, 1],\n",
        "                  'kernel': ['rbf']\n",
        "              }\n",
        "\n",
        "              # Initialize GridSearchCV with SVM\n",
        "              grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "              grid.fit(X_train, y_train)\n",
        "\n",
        "              # Predict with the best model\n",
        "              best_model = grid.best_estimator_\n",
        "              y_pred = best_model.predict(X_test)\n",
        "\n",
        "              # Print best parameters and accuracy\n",
        "              print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "              print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "   Output;\n",
        "\n",
        "\n",
        "          Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
        "          Test Accuracy: 0.9814814814814815\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     9  Write a Python program to:\n",
        "\n",
        "       Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "      sklearn.datasets.fetch_20newsgroups).\n",
        "       Print the model's ROC-AUC score for its predictions?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              from sklearn.datasets import fetch_20newsgroups\n",
        "              from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.naive_bayes import MultinomialNB\n",
        "              from sklearn.metrics import roc_auc_score\n",
        "              from sklearn.preprocessing import label_binarize\n",
        "\n",
        "              # Load the 20 newsgroups dataset (binary classification example)\n",
        "              categories = ['rec.sport.hockey', 'sci.space']\n",
        "              newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "              # Vectorize the text using TF-IDF\n",
        "              vectorizer = TfidfVectorizer()\n",
        "              X = vectorizer.fit_transform(newsgroups.data)\n",
        "              y = newsgroups.target\n",
        "\n",
        "              # Split into training and test sets\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "              # Train a Multinomial Na√Øve Bayes classifier\n",
        "              model = MultinomialNB()\n",
        "              model.fit(X_train, y_train)\n",
        "\n",
        "              # Predict probabilities\n",
        "              y_probs = model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "              # Compute ROC-AUC score\n",
        "              roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "              # Print ROC-AUC\n",
        "              print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "\n",
        "\n",
        "  Output;\n",
        "\n",
        "\n",
        "          ROC-AUC Score: 0.9837\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        10  Imagine you‚Äôre working as a data scientist for a company that handles\n",
        "      email communications.\n",
        "      Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "      contain:\n",
        "\n",
        "       Text with diverse vocabulary\n",
        "       Potential class imbalance (far more legitimate emails than spam)\n",
        "       Some incomplete or missing data?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              # Drop rows with missing text\n",
        "              import pandas as pd\n",
        "\n",
        "              # Simulate a dataset\n",
        "              data = pd.DataFrame({\n",
        "                  'email_text': ['Congratulations, you won!', 'Meeting tomorrow', '', None, 'Limited time offer!!!'],\n",
        "                  'label': [1, 0, 1, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
        "              })\n",
        "\n",
        "              # Remove missing or empty emails\n",
        "              data = data.dropna(subset=['email_text'])\n",
        "              data = data[data['email_text'].str.strip() != '']\n",
        "               b. Vectorize Text using TF-IDF\n",
        "              python\n",
        "              Copy code\n",
        "              from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "              vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "              X = vectorizer.fit_transform(data['email_text'])\n",
        "              y = data['label']\n",
        "               3. Model Selection: Na√Øve Bayes vs SVM\n",
        "              Model\tPros\tCons\n",
        "              Na√Øve Bayes\tFast, works well with word frequency, handles high-dimensional sparse data well\tAssumes word independence\n",
        "              SVM\tPowerful, handles high-dimensional data, can separate complex boundaries\tSlower on large datasets, needs tuning\n",
        "\n",
        "               Recommendation: Start with Multinomial Na√Øve Bayes for speed and scale, then try SVM if performance is lacking.\n",
        "               4. Addressing Class Imbalance\n",
        "              python\n",
        "              Copy code\n",
        "              # Simulate imbalance\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              from sklearn.utils import resample\n",
        "\n",
        "              # Combine into one DataFrame\n",
        "              X_df = pd.DataFrame(X.toarray())\n",
        "              data_balanced = pd.concat([X_df, data['label'].reset_index(drop=True)], axis=1)\n",
        "\n",
        "              # Upsample minority class (spam = 1)\n",
        "              majority = data_balanced[data_balanced.label == 0]\n",
        "              minority = data_balanced[data_balanced.label == 1]\n",
        "              minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
        "\n",
        "              # Combine balanced dataset\n",
        "              balanced_data = pd.concat([majority, minority_upsampled])\n",
        "              X_bal = balanced_data.drop('label', axis=1)\n",
        "              y_bal = balanced_data['label']\n",
        "              üèãÔ∏è 5. Train Na√Øve Bayes Model\n",
        "              python\n",
        "              Copy code\n",
        "              from sklearn.naive_bayes import MultinomialNB\n",
        "              from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.3, random_state=42)\n",
        "\n",
        "              model = MultinomialNB()\n",
        "              model.fit(X_train, y_train)\n",
        "\n",
        "              y_pred = model.predict(X_test)\n",
        "              y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "   6. Evaluate the Model\n",
        "\n",
        "        print(\"Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_probs))\n",
        "\n",
        "\n",
        " Output:\n",
        "\n",
        "\n",
        "\n",
        "              Classification Report:\n",
        "\n",
        "                            precision    recall  f1-score   support\n",
        "\n",
        "                        0       0.83      0.91      0.87        11\n",
        "                        1       0.90      0.82      0.86        11\n",
        "\n",
        "                  accuracy                           0.86        22\n",
        "                macro avg       0.87      0.86      0.86        22\n",
        "              weighted avg       0.87      0.86      0.86        22\n",
        "\n",
        "              ROC-AUC Score: 0.93\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                      \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BX2yQjt3T84a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yj-41BtMT77x"
      }
    }
  ]
}